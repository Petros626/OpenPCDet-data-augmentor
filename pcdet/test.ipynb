{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.kitti.kitti_dataset_custom import *\n",
    "from pcdet.datasets.dataset import *\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/kitti_dataset_custom.yaml')))\n",
    "class_names = ['Car', 'Pedestrian', 'Cyclist']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/kitti/kitti_dataset_custom.py' \n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'kitti'\n",
    "save_path = ROOT_DIR / 'data' / 'kitti'\n",
    "kitti_infos = []\n",
    "num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "def create_kitti_infos(dataset_cfg, class_names, data_path, save_path, workers=4):\n",
    "    from time import sleep\n",
    "\n",
    "    dataset = KittiDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=False, logger=common_utils.create_logger())\n",
    "    \n",
    "    train_split, val_split, test_split = 'train', 'val', 'test'\n",
    "    num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "    train_filename = save_path / ('kitti_infos_%s.pkl' % train_split)\n",
    "    val_filename = save_path / ('kitti_%s_dataset.pkl' % val_split)\n",
    "    trainval_filename = save_path / ('kitti_infos_%s%s.pkl' % (train_split, val_split))\n",
    "    test_filename = save_path / ('kitti_infos_%s.pkl' % test_split)\n",
    "\n",
    "    print('\\n' + '-' * 36 + 'Start to generate data infos' + '-' * 37)\n",
    "    print('---------------CAUTION: Custom code is configured to serve as Augmentor NOT training-----------------')\n",
    "\n",
    "    dataset.set_split(train_split)\n",
    "    # ensure that get_infos() processes the single scene.\n",
    "    # NOTE: get_infos() collects infos about all classes (except 'DontCare'), filter unwanted classes with param `used_classes` in create_groundtruth_database.\n",
    "    kitti_infos_train = dataset.get_infos(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features)\n",
    "    with open(train_filename, 'wb') as f:\n",
    "        pickle.dump(kitti_infos_train, f)\n",
    "    print('Kitti info train file is saved to %s\\n' % train_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    dataset.set_split(val_split)\n",
    "    # ensure that mode 'test' will process the single scene with PointFeatureEncoder, DataProcessor, FOV_FLAG\n",
    "    dataset.training = False\n",
    "    allowed_classes = class_names\n",
    "    kitti_val_dataset = dataset.get_infos_val(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features, class_names=allowed_classes, fov_points_only=False)\n",
    "    with open(val_filename, 'wb') as f:\n",
    "        pickle.dump(kitti_val_dataset, f)\n",
    "    print('Kitti info val file is saved to %s\\n' % val_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    with open(trainval_filename, 'wb') as f:\n",
    "        pickle.dump(kitti_infos_train + kitti_val_dataset, f)\n",
    "    print('Kitti info trainval file is saved to %s\\n' % trainval_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    dataset.set_split(test_split)\n",
    "    kitti_infos_test = dataset.get_infos(num_workers=workers, has_label=False, count_inside_pts=False)\n",
    "    with open(test_filename, 'wb') as f:\n",
    "       pickle.dump(kitti_infos_test, f)\n",
    "    print('Kitti info test file is saved to %s\\n' % test_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    print('\\n---------------Start creating groundtruth database for later data augmentation-------------------------')\n",
    "    print('---------------CAUTION: Custom code is configured to serve as Augmentor NOT training-------------------')\n",
    "    print('---------------No DataProcessor and PointFeatureEncoder required, handled by training data creation----')\n",
    "    \n",
    "    # Input the 'kitti_infos_train.pkl' to generate gt_database (cutted objects of samples)\n",
    "    dataset.set_split(train_split)\n",
    "    dataset.create_groundtruth_database(info_path=train_filename, used_classes=class_names, split=train_split)\n",
    "    print(f'---------------These groundtruth {train_split} objects are randomly inserted into samples (augmentation)-------')\n",
    "    print('-' * 41 + 'Data preparation Done' + '-' * 41)\n",
    "\n",
    "def save_data_list_kitti(data_list=None, save_path=None, root_path=None, sample_id_list=None, augmentors=None):\n",
    "\n",
    "    root_path = root_path if root_path is not None else Path(dataset_cfg.DATA_PATH) \n",
    "    split = dataset_cfg.DATA_SPLIT['train']\n",
    "    split_dir = root_path / 'ImageSets' / (split + '.txt')\n",
    "    sample_id_list = [x.strip() for x in open(split_dir).readlines()] if split_dir.exists() else None\n",
    "    \n",
    "    train_split = 'train'\n",
    "    train_filename = save_path / ('kitti_%s_dataset.pkl' % train_split)\n",
    "\n",
    "    aug_config_list = augmentors\n",
    "    num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "    \n",
    "    print('\\n' + '-' * 35 + 'Start to save data infos(original+augmented)' + '-' * 37)\n",
    "    \n",
    "    with open(train_filename, 'wb') as f:\n",
    "        pickle.dump(data_list, f)\n",
    "\n",
    "    for sample_idx in sample_id_list:\n",
    "        applied_augmentations = [str(name) for name in aug_config_list]\n",
    "        aug_str = ', '.join(applied_augmentations)\n",
    "        print(f\"{split} sample_idx: {sample_idx} (original, {aug_str})\")\n",
    "    \n",
    "    print('Kitti info train/aug file is saved to %s' % train_filename)\n",
    "    print('-' * 49 + 'Data saving Done' + '-' * 51 + '\\n') \n",
    "\n",
    "\n",
    "# Step 1 : Create the data_infos, only validation data_infos and gt_database are important. \n",
    "# The val data gets post-processed through DataProcessor, PointFeatureEncoder, also includes points (w FoV).\n",
    "# The gt_database is necessary for successfully creating augmented training samples.\n",
    "#create_kitti_infos(dataset_cfg, class_names, data_path, save_path, workers=4)\n",
    "\n",
    "# Step 2: Create the training set with data augmentation\n",
    "dataset = KittiDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=True) # the training flag allows data augmentation before training\n",
    "\n",
    "# Step 3: Call the member method to catch information\n",
    "dataset.dataset_w_all_infos = dataset.get_infos(num_workers=4, has_label=True, count_inside_pts=True, num_features=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Step 4: save it\n",
    "dataset_as_list = []\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    data, applied_augmentors = dataset[idx]\n",
    "    # debug\n",
    "    #sample_idx = data[0]['frame_id']\n",
    "    #print(f\"{sample_idx}\")\n",
    "    dataset_as_list.append(data)   \n",
    "    # dataset_as_list.append(dataset[idx])\n",
    "\n",
    "#gc.collect()\n",
    "\n",
    "save_data_list_kitti(data_list=dataset_as_list, save_path=save_path, root_path=None, sample_id_list=None, augmentors=applied_augmentors)\n",
    "\n",
    "# clean up variables after saving\n",
    "#del dataset, dataset_as_list\n",
    "#del data, applied_augmentors\n",
    "#gc.collect()\n",
    "\n",
    "# clean up variables in notebook & # restart ipython kernel\n",
    "#if get_ipython():\n",
    "#    get_ipython().run_line_magic('reset', '-sf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom with Densification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - NAME: range_based_densification\n",
    "#           NUM_POINT_COPIES: 1\n",
    "#           DELTA_R_RANGE: [0.1, 0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom with Upsampling Griesbacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - NAME: random_beam_upsample_griesbacher\n",
    "#           BEAM_UPSAMPLE_PROB: 1 # upsample all point clouds\n",
    "#           PHI_THRESHOLD: 0.001570796 # KITTI: 1/1273*2 rad ≈ 0.001570796 ≈ 0.09° (HDL-64E data sheet) ; ZOD: 1/300*2 rad ≈ 0.006666667 ≈ 0.38° (VLS128 data sheet),\n",
    "#           R_THRESHOLD: 2.0 # meters\n",
    "#           NUM_INTERP_BEAMS: 1 # all_beams ≈ B + (B - 1) * num_interp_beams so for B = 64 (HDL-64E) and num_interp_beams = 1 -> 64 + (64- 1) = 127 (close to 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom with Paper Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - NAME: d2_range_image_4ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ZODDatasetCustom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.zod.zod_dataset_custom import *\n",
    "from pcdet.datasets.dataset import *\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/zod_dataset_custom.yaml')))\n",
    "class_names = ['Vehicle_Car', 'Pedestrian', 'VulnerableVehicle_Bicycle']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/zod/zod_dataset_custom.py' \n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'zod'\n",
    "save_path = ROOT_DIR / 'data' / 'zod'\n",
    "zod_infos = []\n",
    "num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "def create_zod_infos(dataset_cfg, class_names, data_path, save_path, workers=4):\n",
    "    from time import sleep\n",
    "\n",
    "    dataset = ZODDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=False, logger=common_utils.create_logger(), creating_pkl_infos=True)\n",
    "    \n",
    "    train_split, val_split = 'train', 'val'\n",
    "    version = 'full'\n",
    "    num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "    train_filename = save_path / ('zod_infos_%s_%s.pkl' % (train_split, version))\n",
    "    val_filename = save_path / ('zod_%s_dataset.pkl' % val_split)\n",
    "    trainval_filename = save_path / ('zod_infos_trainval_%s.pkl' % version)\n",
    "\n",
    "    print('\\n' + '-' * 36 + 'Start to generate data infos' + '-' * 37)\n",
    "    print('---------------CAUTION: Custom code is configured to serve as Augmentor NOT training-----------------')\n",
    "\n",
    "    dataset.set_split(train_split, version)\n",
    "    zod_infos_train = dataset.get_infos(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features)\n",
    "    with open(train_filename, 'wb') as f:\n",
    "        pickle.dump(zod_infos_train, f)\n",
    "    print('Zod info train file is saved to %s\\n' % train_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    dataset.set_split(val_split, version)\n",
    "    # ensure that mode 'test' will process the single scene with PointFeatureEncoder, DataProcessor, FOV_FLAG\n",
    "    dataset.training = False\n",
    "    allowed_classes = class_names\n",
    "    zod_val_dataset = dataset.get_infos_val(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features, class_names=allowed_classes)\n",
    "    with open(val_filename, 'wb') as f:\n",
    "        pickle.dump(zod_val_dataset, f)\n",
    "    print('Zod info val file is saved to %s\\n' % val_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    with open(trainval_filename, 'wb') as f:\n",
    "        pickle.dump(zod_infos_train + zod_val_dataset, f)\n",
    "    print('Zod info trainval file is saved to %s\\n' % trainval_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    print('\\n---------------Start creating groundtruth database for later data augmentation-------------------------')\n",
    "    print('---------------CAUTION: Custom code is configured to serve as Augmentor NOT training-------------------')\n",
    "    print('---------------No DataProcessor and PointFeatureEncoder required, handled by training data creation----')\n",
    "\n",
    "    # Input the 'zod_infos_train_full.pkl' to generate gt_database (cutted objects of samples)\n",
    "    dataset.set_split(train_split, version)\n",
    "    dataset.create_groundtruth_database(info_path=train_filename, version=version, used_classes=class_names, split=train_split)\n",
    "    print(f'---------------These groundtruth {train_split} objects are randomly inserted into samples (augmentation)-------')\n",
    "    print('-' * 41 + 'Data preparation Done' + '-' * 41)\n",
    "\n",
    "def save_data_list_zod(data_list=None, save_path=None, root_path=None, sample_id_list=None, augmentors=None):\n",
    "    #import blosc\n",
    "    import zstandard as zstd\n",
    "\n",
    "    root_path = root_path if root_path is not None else Path(dataset_cfg.DATA_PATH) \n",
    "    split = dataset_cfg.DATA_SPLIT['train']\n",
    "    \n",
    "    train_split = 'train'\n",
    "    train_filename = save_path / ('zod_%s_dataset.pkl.zst' % train_split)\n",
    "\n",
    "    aug_config_list = augmentors\n",
    "    num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "    \n",
    "    print('\\n' + '-' * 35 + 'Start to save data infos(original+augmented)' + '-' * 37)\n",
    "    \n",
    "    # Funktioniert 100 %\n",
    "    # raw = pickle.dumps(data_list, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # cctx = zstd.ZstdCompressor(level=15, threads=-1)\n",
    "    # comp = cctx.compress(raw)\n",
    "    # with open(train_filename, 'wb') as f:\n",
    "    #     f.write(comp)\n",
    "    \n",
    "    cctx = zstd.ZstdCompressor(level=19, threads=-1)\n",
    "    with open(train_filename, 'wb') as fh, cctx.stream_writer(fh) as zfh:\n",
    "        pickler = pickle.Pickler(zfh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickler.dump(data_list)\n",
    "        pickler.clear_memo()\n",
    "\n",
    "    # for sample_idx in sample_id_list:\n",
    "    #     applied_augmentations = [str(name) for name in aug_config_list]\n",
    "    #     aug_str = ', '.join(applied_augmentations)\n",
    "    #     print(f\"{split} sample_idx: {sample_idx} (original, {aug_str})\")\n",
    "    \n",
    "    print('Zod info train/aug file is saved to %s' % train_filename)\n",
    "    print('-' * 49 + 'Data saving Done' + '-' * 51 + '\\n') \n",
    "\n",
    "\n",
    "# Step 1 : Create the data_infos, only validatiosn data_infos and gt_database are important. \n",
    "# The val data gets post-processed through DataProcessor, PointFeatureEncoder, also includes points (w FoV).\n",
    "# The gt_database is necessary for successfully creating augmented training samples.\n",
    "#create_zod_infos(dataset_cfg, class_names, data_path, save_path, workers=4)\n",
    "\n",
    "# Step 2: Create the training set with data augmentation\n",
    "dataset = ZODDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=True, logger=common_utils.create_logger(), creating_pkl_infos=False)\n",
    "\n",
    "# Step 3: Call the member method to catch information\n",
    "train_split, val_split = 'train', 'val'\n",
    "version = 'full'\n",
    "train_filename = data_path / ('zod_infos_%s_%s.pkl' % (train_split, version))\n",
    "with open(train_filename, 'rb') as f:\n",
    "    zod_infos_train_full = pickle.load(f)\n",
    "\n",
    "sample_id_list = [info['point_cloud']['lidar_idx'] for info in zod_infos_train_full]\n",
    "\n",
    "dataset.dataset_w_all_infos = dataset.get_infos(num_workers=4, has_label=True, count_inside_pts=True, sample_id_list=sample_id_list, num_features=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: save it (old, slower, OOM danger)\n",
    "#dataset_as_list = []\n",
    "\n",
    "#for idx in range(len(dataset)):\n",
    "#    data, applied_augmentors = dataset[idx]\n",
    "    # debug\n",
    "    #sample_idx = data[0]['frame_id']\n",
    "    #print(f\"{sample_idx}\")\n",
    "#    dataset_as_list.append(data)   \n",
    "    # dataset_as_list.append(dataset[idx])\n",
    "\n",
    "#save_data_list_zod(data_list=dataset_as_list, save_path=save_path, root_path=None, sample_id_list=sample_id_list, augmentors=applied_augmentors)\n",
    "\n",
    "# Step 4: save it (new) \n",
    "save_data_list_zod(\n",
    "    data_list=[dataset[i][0] for i in range(len(dataset))],\n",
    "    save_path=save_path,\n",
    "    root_path=None,\n",
    "    sample_id_list=sample_id_list,\n",
    "    augmentors=[cfg.get('NAME', str(cfg)) for cfg in dataset_cfg.DATA_AUGMENTOR.AUG_CONFIG_LIST],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import zstandard as zstd\n",
    "\n",
    "train_filename = '/home/rlab10/OpenPCDet/data/zod/zod_train_dataset.pkl.zst'\n",
    "with open(train_filename, 'rb') as f, zstd.ZstdDecompressor().stream_reader(f) as zfh:\n",
    "    data_list = pickle.load(zfh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Compression for pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: save it (new, faster)\n",
    "\n",
    "#save_data_list_zod_streaming(dataset=dataset, save_path=save_path, root_path=None, sample_id_list=sample_id_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_data_list_zod_streaming(dataset=None, save_path=None, root_path=None, sample_id_list=None):\n",
    "#     import zstandard as zstd\n",
    "#     import pickle\n",
    "\n",
    "#     root_path = root_path if root_path is not None else Path(dataset_cfg.DATA_PATH) \n",
    "#     split = dataset_cfg.DATA_SPLIT['train']\n",
    "    \n",
    "#     train_split = 'train'\n",
    "#     train_filename = save_path / ('zod_%s_dataset.pkl.zst' % train_split)\n",
    "\n",
    "#     print('\\n' + '-' * 35 + 'Start to save data infos(original+augmented)' + '-' * 37)\n",
    "    \n",
    "#     cctx = zstd.ZstdCompressor(level=5, threads=-1, write_checksum=False, )\n",
    "#     with open(train_filename, 'wb') as fh, cctx.stream_writer(fh) as zfh:\n",
    "#         pickler = pickle.Pickler(zfh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#         for i in range(len(dataset)):\n",
    "#             data, applied_augmentors = dataset[i]  # triggert __getitem__ + Augmentierung\n",
    "#             pickler.dump(data)\n",
    "        \n",
    "#     if sample_id_list is not None:\n",
    "#         for sample_idx in sample_id_list:\n",
    "#             aug_str = ', '.join(map(str, applied_augmentors))\n",
    "#             print(f\"{split} sample_idx: {sample_idx} (original, {aug_str})\")\n",
    "    \n",
    "#     print('Zod info train/aug file is saved to %s' % train_filename)\n",
    "#     print('-' * 49 + 'Data saving Done' + '-' * 51 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug with MultiProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/microsoft/debugpy/issues/1168#issuecomment-1377998813\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.modules['debugpy'].__file__\n",
    "\n",
    "# go to '/home/rlab10/anaconda3/envs/pcdet/lib/python3.11/site-packages/debugpy\n",
    "# find ebugpy/server/api.py\n",
    "# change \"subProcess\": True to \"subProcess\": False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcdet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
