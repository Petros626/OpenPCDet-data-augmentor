{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.kitti.kitti_dataset_custom import *\n",
    "from pcdet.datasets.dataset import *\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/kitti_dataset_custom.yaml')))\n",
    "class_names = ['Car', 'Pedestrian', 'Cyclist']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/kitti/kitti_dataset_custom.py' \n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'kitti'\n",
    "save_path = ROOT_DIR / 'data' / 'kitti'\n",
    "kitti_infos = []\n",
    "num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "def create_kitti_infos(dataset_cfg, class_names, data_path, save_path, workers=4):\n",
    "    from time import sleep\n",
    "\n",
    "    dataset = KittiDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=False, logger=common_utils.create_logger())\n",
    "    \n",
    "    train_split, val_split, test_split = 'train', 'val', 'test'\n",
    "    num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "    train_filename = save_path / ('kitti_infos_%s.pkl' % train_split)\n",
    "    val_filename = save_path / ('kitti_%s_dataset.pkl' % val_split)\n",
    "    trainval_filename = save_path / ('kitti_infos_%s%s.pkl' % (train_split, val_split))\n",
    "    test_filename = save_path / ('kitti_infos_%s.pkl' % test_split)\n",
    "\n",
    "    print('\\n' + '-' * 36 + 'Start to generate data infos' + '-' * 37)\n",
    "    print('---------------CAUTION: Custom code is configured to serve as Augmentor NOT training-----------------')\n",
    "\n",
    "    dataset.set_split(train_split)\n",
    "    # ensure that get_infos() processes the single scene.\n",
    "    # NOTE: get_infos() collects infos about all classes (except 'DontCare'), filter unwanted classes with param `used_classes` in create_groundtruth_database.\n",
    "    kitti_infos_train = dataset.get_infos(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features)\n",
    "    with open(train_filename, 'wb') as f:\n",
    "        pickle.dump(kitti_infos_train, f)\n",
    "    print('Kitti info train file is saved to %s\\n' % train_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    dataset.set_split(val_split)\n",
    "    # ensure that mode 'test' will process the single scene with PointFeatureEncoder, DataProcessor, FOV_FLAG\n",
    "    dataset.training = False\n",
    "    allowed_classes = class_names\n",
    "    kitti_val_dataset = dataset.get_infos_val(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features, class_names=allowed_classes, fov_points_only=False)\n",
    "    with open(val_filename, 'wb') as f:\n",
    "        pickle.dump(kitti_val_dataset, f)\n",
    "    print('Kitti info val dataset is saved to %s\\n' % val_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    with open(trainval_filename, 'wb') as f:\n",
    "        pickle.dump(kitti_infos_train + kitti_val_dataset, f)\n",
    "    print('Kitti info trainval file is saved to %s\\n' % trainval_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    dataset.set_split(test_split)\n",
    "    kitti_infos_test = dataset.get_infos(num_workers=workers, has_label=False, count_inside_pts=False)\n",
    "    with open(test_filename, 'wb') as f:\n",
    "       pickle.dump(kitti_infos_test, f)\n",
    "    print('Kitti info test file is saved to %s\\n' % test_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    print('\\n---------------Start creating groundtruth database for later data augmentation-------------------------')\n",
    "    print('---------------CAUTION: Custom code is configured to serve as Augmentor NOT training-------------------')\n",
    "    print('---------------No DataProcessor and PointFeatureEncoder required, handled by training data creation----')\n",
    "    \n",
    "    # Input the 'kitti_infos_train.pkl' to generate gt_database (cutted objects of samples)\n",
    "    dataset.set_split(train_split)\n",
    "    dataset.create_groundtruth_database(info_path=train_filename, used_classes=class_names, split=train_split)\n",
    "    print(f'---------------These groundtruth {train_split} objects are randomly inserted into samples (augmentation)-------')\n",
    "    print('-' * 41 + 'Data preparation Done' + '-' * 41)\n",
    "\n",
    "def save_data_list_kitti(data_list=None, save_path=None, root_path=None, sample_id_list=None, augmentors=None):\n",
    "\n",
    "    root_path = root_path if root_path is not None else Path(dataset_cfg.DATA_PATH) \n",
    "    split = dataset_cfg.DATA_SPLIT['train']\n",
    "    split_dir = root_path / 'ImageSets' / (split + '.txt')\n",
    "    #sample_id_list = [x.strip() for x in open(split_dir).readlines()] if split_dir.exists() else None\n",
    "    \n",
    "    train_split = 'train'\n",
    "    train_filename = save_path / ('kitti_%s_dataset.pkl' % train_split)\n",
    "\n",
    "    #aug_config_list = augmentors\n",
    "    #num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "    \n",
    "    print('\\n' + '-' * 35 + 'Start to save data infos(original+augmented)' + '-' * 37)\n",
    "    \n",
    "    with open(train_filename, 'wb') as f:\n",
    "        pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickler.dump(data_list)\n",
    "        pickler.clear_memo()\n",
    "\n",
    "    #for sample_idx in sample_id_list:\n",
    "    #    applied_augmentations = [str(name) for name in aug_config_list]\n",
    "    #    aug_str = ', '.join(applied_augmentations)\n",
    "    #    print(f\"{split} sample_idx: {sample_idx} (original, {aug_str})\")\n",
    "    \n",
    "    print('Kitti info train/aug file is saved to %s' % train_filename)\n",
    "    print('-' * 49 + 'Data saving Done' + '-' * 51 + '\\n') \n",
    "\n",
    "\n",
    "# Step 1 : Create the data_infos, only validation data_infos and gt_database are important. \n",
    "# The val data gets post-processed through DataProcessor, PointFeatureEncoder, also includes points (w FoV).\n",
    "# The gt_database is necessary for successfully creating augmented training samples.\n",
    "#create_kitti_infos(dataset_cfg, class_names, data_path, save_path, workers=4)\n",
    "\n",
    "# Step 2: Create the training set with data augmentation\n",
    "dataset = KittiDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=True, logger=common_utils.create_logger()) # the training flag allows data augmentation before training\n",
    "\n",
    "# Step 3: Call the member method to catch information\n",
    "dataset.dataset_w_all_infos = dataset.get_infos(num_workers=4, has_label=True, count_inside_pts=True, num_features=num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save KittiDatasetCustom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Step 4: save it\n",
    "dataset_as_list = []\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    data, applied_augmentors = dataset[idx]\n",
    "    # debug\n",
    "    #sample_idx = data[0]['frame_id']\n",
    "    #print(f\"{sample_idx}\")\n",
    "    dataset_as_list.append(data)   \n",
    "    # dataset_as_list.append(dataset[idx])\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "save_data_list_kitti(data_list=dataset_as_list, save_path=save_path, root_path=None, sample_id_list=None, augmentors=applied_augmentors)\n",
    "\n",
    "# clean up variables after saving\n",
    "del dataset, dataset_as_list\n",
    "del data, applied_augmentors\n",
    "gc.collect()\n",
    "\n",
    "# clean up variables in notebook & # restart ipython kernel\n",
    "if get_ipython():\n",
    "    get_ipython().run_line_magic('reset', '-sf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom with range-based densification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.augmentor.data_augmentor import DataAugmentor\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/DG_KITTI/kitti_dataset_custom_densification.yaml')))\n",
    "class_names = ['Car', 'Pedestrian', 'Cyclist']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/kitti/kitti_dataset_custom.py'\n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'kitti' # raw data path\n",
    "save_path = ROOT_DIR / 'data' / 'kitti' / 'Domain Generalization' / 'densification'\n",
    "\n",
    "aug_cfg = dataset_cfg.DATA_AUGMENTOR['AUG_CONFIG_LIST'][0]\n",
    "name = aug_cfg.get('NAME')\n",
    "num_point_copies = aug_cfg.get('NUM_POINT_COPIES', 3)\n",
    "delta_r = aug_cfg.get('DELTA_R_RANGE', [0.05, 0.1])\n",
    "\n",
    "train_split, val_split = 'train', 'val'\n",
    "train_filename = data_path / ('kitti_%s_dataset.pkl' % train_split)\n",
    "dens_train_filename = save_path / ('kitti_%s_dataset_densified_%s.pkl' % (train_split, num_point_copies))\n",
    "val_filename = data_path / ('kitti_%s_dataset.pkl' % val_split)\n",
    "dens_val_filename = save_path / ('kitti_%s_dataset_densified_%s.pkl' % (val_split, num_point_copies))\n",
    "\n",
    "augmentor = DataAugmentor(root_path=data_path, augmentor_configs=dataset_cfg.DATA_AUGMENTOR, class_names=class_names, logger=common_utils.create_logger())\n",
    "if num_point_copies and delta_r and augmentor.logger is not None:\n",
    "    augmentor.logger.info('Range based densification enabled with Δr %s and num_copies %d' % (str(delta_r), num_point_copies))\n",
    "\n",
    "# training \n",
    "with open(train_filename, 'rb') as f:\n",
    "    data_list = pickle.load(f)\n",
    "\n",
    "for sample in tqdm(data_list, desc=\"Samples\"):\n",
    "    #print(f\"Processing frame_id: {sample[0].get('frame_id', 'N/A')}\")\n",
    "    for i, data_dict in enumerate(sample):\n",
    "        for aug_func in augmentor.data_augmentor_queue:\n",
    "            sample[i] = aug_func(data_dict)\n",
    "\n",
    "with open(dens_train_filename, 'wb') as f:\n",
    "        pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickler.dump(data_list)\n",
    "        print('Kitti info train dataset densified is saved to %s\\n' % dens_train_filename)\n",
    "        pickler.clear_memo()\n",
    "\n",
    "# empty RAM\n",
    "del data_list\n",
    "gc.collect()\n",
    "\n",
    "# validation\n",
    "with open(val_filename, 'rb') as f:\n",
    "     data_list = pickle.load(f)\n",
    "\n",
    "for sample in tqdm(data_list, desc=\"Samples\"):\n",
    "        for aug_func in augmentor.data_augmentor_queue:\n",
    "            sample = aug_func(sample)\n",
    "\n",
    "with open(dens_val_filename, 'wb') as f:\n",
    "        pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickler.dump(data_list)\n",
    "        print('Kitti info val dataset densified is saved to %s\\n' % dens_val_filename)\n",
    "        pickler.clear_memo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom with Diode IDs (preparatory work for RBRS (see below))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.kitti.kitti_dataset_custom import *\n",
    "from pcdet.datasets.dataset import *\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/DG_KITTI/kitti_dataset_custom_rbrs_prep.yaml')))\n",
    "class_names = ['Car', 'Pedestrian', 'Cyclist']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/kitti/kitti_dataset_custom.py' \n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'kitti' # raw data path\n",
    "save_path = ROOT_DIR / 'data' / 'kitti'\n",
    "kitti_infos = []\n",
    "num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "def create_kitti_infos(dataset_cfg, class_names, data_path, save_path, workers=4):\n",
    "    from time import sleep\n",
    "\n",
    "    dataset = KittiDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=False, logger=common_utils.create_logger())\n",
    "    \n",
    "    train_split, val_split = 'train', 'val'\n",
    "    num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "    train_filename = data_path / ('kitti_infos_%s.pkl' % train_split)\n",
    "    val_filename = save_path / ('kitti_%s_dataset_beamlabels.pkl' % val_split)\n",
    "\n",
    "    print('\\n' + '-' * 36 + 'Start to generate data infos with beamlabels' + '-' * 37)\n",
    "    print('---------------CAUTION: Custom code is configured to prepare RBRS-----------------')\n",
    "\n",
    "    dataset.set_split(val_split)\n",
    "    dataset.training = False\n",
    "    allowed_classes = class_names\n",
    "    kitti_val_dataset = dataset.get_infos_val(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features, class_names=allowed_classes, fov_points_only=False, with_beam_label=True)\n",
    "    with open(val_filename, 'wb') as f:\n",
    "        pickle.dump(kitti_val_dataset, f)\n",
    "    print('Kitti info val dataset with beamlabels is saved to %s\\n' % val_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    # print('\\n---------------Start creating groundtruth database with beamlabels for RBRS-------------------------')\n",
    "    # print('---------------CAUTION: Custom code is configured to serve as Upsampling NOT training-------------------')\n",
    "    # print('---------------No DataProcessor and PointFeatureEncoder required, handled by training data creation----')\n",
    "    \n",
    "    # # Input the 'kitti_infos_train_beamlabels.pkl' to generate gt_database (cutted objects of samples)\n",
    "    # dataset.set_split(train_split)\n",
    "    # dataset.create_groundtruth_database(info_path=train_filename, used_classes=class_names, split=train_split, with_beam_labels=True)\n",
    "    # print(f'---------------These groundtruth {train_split} objects are randomly inserted into samples (augmentation)-------')\n",
    "    # print('-' * 41 + 'Data preparation Done' + '-' * 41)\n",
    "\n",
    "def save_data_list_kitti(data_list=None, save_path=None, root_path=None, sample_id_list=None, augmentors=None):\n",
    "\n",
    "    root_path = root_path if root_path is not None else Path(dataset_cfg.DATA_PATH) \n",
    "    split = dataset_cfg.DATA_SPLIT['train']\n",
    "    split_dir = root_path / 'ImageSets' / (split + '.txt')\n",
    "    #sample_id_list = [x.strip() for x in open(split_dir).readlines()] if split_dir.exists() else None\n",
    "    \n",
    "    train_split = 'train'\n",
    "    train_filename = save_path / ('kitti_%s_dataset_beamlabels.pkl' % train_split)\n",
    "\n",
    "    #aug_config_list = augmentors\n",
    "    #num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "    \n",
    "    print('\\n' + '-' * 35 + 'Start to save data infos(original+augmented)' + '-' * 37)\n",
    "    \n",
    "    with open(train_filename, 'wb') as f:\n",
    "        pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickler.dump(data_list)\n",
    "        pickler.clear_memo()\n",
    "\n",
    "    #for sample_idx in sample_id_list:\n",
    "    #    applied_augmentations = [str(name) for name in aug_config_list]\n",
    "    #    aug_str = ', '.join(applied_augmentations)\n",
    "    #    print(f\"{split} sample_idx: {sample_idx} (original, {aug_str})\")\n",
    "    \n",
    "    print('Kitti info train/aug file with beamlabels is saved to %s' % train_filename)\n",
    "    print('-' * 49 + 'Data saving Done' + '-' * 51 + '\\n') \n",
    "\n",
    "\n",
    "# Step 1 : Create the data_infos, only validation data_infos and gt_database are important. \n",
    "# The val data gets post-processed through DataProcessor, PointFeatureEncoder, also includes points\n",
    "# The gt_database is necessary for successfully creating upsampled training samples.\n",
    "create_kitti_infos(dataset_cfg, class_names, data_path, save_path, workers=4)\n",
    "\n",
    "# Step 2: Create the training set with beamlabels\n",
    "#dataset = KittiDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=True, logger=common_utils.create_logger()) # the training flag allows data augmentation before training\n",
    "\n",
    "# Step 3: Call the member method to catch information\n",
    "#dataset.dataset_w_all_infos = dataset.get_infos(num_workers=4, has_label=True, count_inside_pts=True, num_features=num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save KittiDatasetCustom with Diode IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: save it (new) \n",
    "save_data_list_kitti(\n",
    "    data_list=[dataset[i][0] for i in range(len(dataset.dataset_w_all_infos))], # dataset.dataset_w_all_infos\n",
    "    save_path=save_path,\n",
    "    root_path=None,\n",
    "    sample_id_list=None,\n",
    "    augmentors=[cfg.get('NAME', str(cfg)) for cfg in dataset_cfg.DATA_AUGMENTOR.AUG_CONFIG_LIST],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom with Random Beam Re-Sampling (RBRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.augmentor.data_augmentor import DataAugmentor\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/DG_KITTI/kitti_dataset_custom_rbrs.yaml')))\n",
    "class_names = ['Car', 'Pedestrian', 'Cyclist']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/kitti/kitti_dataset_custom.py'\n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'kitti'\n",
    "save_path = ROOT_DIR / 'data' / 'kitti' / 'Domain Generalization' / 'random beam re-sampling'\n",
    "\n",
    "train_split, val_split = 'train', 'val'\n",
    "train_filename = data_path / ('kitti_%s_dataset_beamlabels.pkl' % train_split)\n",
    "rbrs_train_filename = save_path / ('kitti_%s_dataset_rbrs.pkl' % train_split)\n",
    "val_filename = data_path / ('kitti_%s_dataset_beamlabels.pkl' % val_split)\n",
    "rbrs_val_filename = save_path / ('kitti_%s_dataset_rbrs.pkl' % val_split)\n",
    "\n",
    "aug_cfg = dataset_cfg.DATA_AUGMENTOR['AUG_CONFIG_LIST'][0]\n",
    "name = aug_cfg.get('NAME')\n",
    "upsampling_prob = aug_cfg.get('BEAM_UPSAMPLE_PROB', 1)\n",
    "phi = aug_cfg.get('PHI_THRESHOLD', 0.001570796)\n",
    "range = aug_cfg.get('R_THRESHOLD', 2.0)\n",
    "num_interp_beams = aug_cfg.get('NUM_INTERP_BEAMS', 1)\n",
    "\n",
    "augmentor = DataAugmentor(root_path=data_path, augmentor_configs=dataset_cfg.DATA_AUGMENTOR, class_names=class_names, logger=common_utils.create_logger())\n",
    "if phi and range and augmentor.logger is not None:\n",
    "    augmentor.logger.info('Random beam re-sampling enabled with upsample prob: %s, φ %s, r: %s and num. interp. beams: %s' % (upsampling_prob, phi, range, num_interp_beams))\n",
    "\n",
    "# # training \n",
    "# with open(train_filename, 'rb') as f:\n",
    "#     data_list = pickle.load(f)\n",
    "\n",
    "# for sample in tqdm(data_list, desc=\"Samples\"):\n",
    "#     #print(f\"Processing frame_id: {sample[0].get('frame_id', 'N/A')}\")\n",
    "#     for i, data_dict in enumerate(sample):\n",
    "#         for aug_func in augmentor.data_augmentor_queue:\n",
    "#             sample[i] = aug_func(data_dict)\n",
    "\n",
    "# with open(rbrs_train_filename, 'wb') as f:\n",
    "#         pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#         pickler.dump(data_list)\n",
    "#         print('Kitti info train dataset upsampled is saved to %s\\n' % rbrs_train_filename)\n",
    "#         pickler.clear_memo()\n",
    "\n",
    "# empty RAM\n",
    "#del data_list\n",
    "#gc.collect()\n",
    "\n",
    "# validation\n",
    "with open(val_filename, 'rb') as f:\n",
    "     data_list = pickle.load(f)\n",
    "\n",
    "for sample in tqdm(data_list, desc=\"Samples\"):\n",
    "        for aug_func in augmentor.data_augmentor_queue:\n",
    "            sample = aug_func(sample)\n",
    "\n",
    "with open(rbrs_val_filename, 'wb') as f:\n",
    "        pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickler.dump(data_list)\n",
    "        print('Kitti info val dataset upsampled is saved to %s\\n' % rbrs_val_filename)\n",
    "        pickler.clear_memo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom with Paper Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - NAME: d2_range_image_4ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ZODDatasetCustom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.zod.zod_dataset_custom import *\n",
    "from pcdet.datasets.dataset import *\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/zod_dataset_custom.yaml')))\n",
    "class_names = ['Vehicle_Car', 'Pedestrian', 'VulnerableVehicle_Bicycle']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/zod/zod_dataset_custom.py' \n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'zod'\n",
    "save_path = ROOT_DIR / 'data' / 'zod'\n",
    "zod_infos = []\n",
    "num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "def create_zod_infos(dataset_cfg, class_names, data_path, save_path, workers=4):\n",
    "    from time import sleep\n",
    "\n",
    "    dataset = ZODDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=False, logger=common_utils.create_logger(), creating_pkl_infos=True)\n",
    "    \n",
    "    train_split, val_split = 'train', 'val'\n",
    "    version = 'full'\n",
    "    num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "    train_filename = save_path / ('zod_infos_%s_%s.pkl' % (train_split, version))\n",
    "    val_filename = save_path / ('zod_%s_dataset.pkl' % val_split)\n",
    "    trainval_filename = save_path / ('zod_infos_trainval_%s.pkl' % version)\n",
    "\n",
    "    print('\\n' + '-' * 36 + 'Start to generate data infos' + '-' * 37)\n",
    "    print('---------------CAUTION: Custom code is configured to serve as Augmentor NOT training-----------------')\n",
    "\n",
    "    dataset.set_split(train_split, version)\n",
    "    zod_infos_train = dataset.get_infos(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features)\n",
    "    with open(train_filename, 'wb') as f:\n",
    "        pickle.dump(zod_infos_train, f)\n",
    "    print('Zod info train file is saved to %s\\n' % train_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    dataset.set_split(val_split, version)\n",
    "    # ensure that mode 'test' will process the single scene with PointFeatureEncoder, DataProcessor, FOV_FLAG\n",
    "    dataset.training = False\n",
    "    zod_val_dataset = dataset.get_infos_val(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features)\n",
    "    with open(val_filename, 'wb') as f:\n",
    "        pickle.dump(zod_val_dataset, f)\n",
    "    print('Zod info val file is saved to %s\\n' % val_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    with open(trainval_filename, 'wb') as f:\n",
    "        pickle.dump(zod_infos_train + zod_val_dataset, f)\n",
    "    print('Zod info trainval file is saved to %s\\n' % trainval_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    print('\\n---------------Start creating groundtruth database for later data augmentation-------------------------')\n",
    "    print('---------------CAUTION: Custom code is configured to serve as Augmentor NOT training-------------------')\n",
    "    print('---------------No DataProcessor and PointFeatureEncoder required, handled by training data creation----')\n",
    "\n",
    "    # Input the 'zod_infos_train_full.pkl' to generate gt_database (cutted objects of samples)\n",
    "    dataset.set_split(train_split, version)\n",
    "    dataset.create_groundtruth_database(info_path=train_filename, version=version, split=train_split)\n",
    "    print(f'---------------These groundtruth {train_split} objects are randomly inserted into samples (augmentation)-------')\n",
    "    print('-' * 41 + 'Data preparation Done' + '-' * 41)\n",
    "\n",
    "def save_data_list_zod(data_list=None, save_path=None, root_path=None, sample_id_list=None, augmentors=None):\n",
    "    #import blosc\n",
    "    #import zstandard as zstd\n",
    "\n",
    "    root_path = root_path if root_path is not None else Path(dataset_cfg.DATA_PATH) \n",
    "    split = dataset_cfg.DATA_SPLIT['train']\n",
    "    \n",
    "    train_split = 'train'\n",
    "    train_filename = save_path / ('zod_%s_dataset.pkl' % train_split)\n",
    "\n",
    "    aug_config_list = augmentors\n",
    "    num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "    \n",
    "    print('\\n' + '-' * 35 + 'Start to save data infos(original+augmented)' + '-' * 37)\n",
    "    \n",
    "    # experimental\n",
    "    # raw = pickle.dumps(data_list, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # cctx = zstd.ZstdCompressor(level=15, threads=-1)\n",
    "    # comp = cctx.compress(raw)\n",
    "    # with open(train_filename, 'wb') as f:\n",
    "    #     f.write(comp)\n",
    "    \n",
    "    # experimental\n",
    "    #cctx = zstd.ZstdCompressor(level=3, threads=-1)\n",
    "    #with open(train_filename, 'wb') as fh, cctx.stream_writer(fh) as zfh:\n",
    "    #    pickler = pickle.Pickler(zfh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #    pickler.dump(data_list)\n",
    "    #    pickler.clear_memo()\n",
    "    \n",
    "    with open(train_filename, 'wb') as f:\n",
    "        pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickler.dump(data_list)\n",
    "        pickler.clear_memo()\n",
    "    \n",
    "    # for sample_idx in sample_id_list:\n",
    "    #     applied_augmentations = [str(name) for name in aug_config_list]\n",
    "    #     aug_str = ', '.join(applied_augmentations)\n",
    "\n",
    "    #     print(f\"{split} sample_idx: {sample_idx} (original, {aug_str})\")\n",
    "    \n",
    "    print('Zod info train/aug file is saved to %s' % train_filename)\n",
    "    print('-' * 49 + 'Data saving Done' + '-' * 51 + '\\n') \n",
    "\n",
    "\n",
    "# Step 1 : Create the data_infos, only validatiosn data_infos and gt_database are important. \n",
    "# The val data gets post-processed through DataProcessor, PointFeatureEncoder, also includes points (w FoV).\n",
    "# The gt_database is necessary for successfully creating augmented training samples.\n",
    "#create_zod_infos(dataset_cfg, class_names, data_path, save_path, workers=4)\n",
    "\n",
    "# Step 2: Create the training set with data augmentation\n",
    "dataset = ZODDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=True, logger=common_utils.create_logger(), creating_pkl_infos=False)\n",
    "\n",
    "# Step 3: Call the member method to catch information\n",
    "train_split, val_split = 'train', 'val'\n",
    "version = 'full'\n",
    "train_filename = data_path / ('zod_infos_%s_%s.pkl' % (train_split, version))\n",
    "with open(train_filename, 'rb') as f:\n",
    "    zod_infos_train_full = pickle.load(f)\n",
    "\n",
    "sample_id_list = [info['point_cloud']['lidar_idx'] for info in zod_infos_train_full]\n",
    "\n",
    "dataset.dataset_w_all_infos = dataset.get_infos(num_workers=4, has_label=True, count_inside_pts=True, sample_id_list=sample_id_list, num_features=num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: save it (new) \n",
    "save_data_list_zod(\n",
    "    data_list=[dataset[i][0] for i in range(len(dataset.dataset_w_all_infos))], # dataset.dataset_w_all_infos\n",
    "    save_path=save_path,\n",
    "    root_path=None,\n",
    "    sample_id_list=sample_id_list,\n",
    "    augmentors=[cfg.get('NAME', str(cfg)) for cfg in dataset_cfg.DATA_AUGMENTOR.AUG_CONFIG_LIST],\n",
    ")\n",
    "\n",
    "# Step 4: save it (old, slower, OOM danger)\n",
    "#dataset_as_list = []\n",
    "\n",
    "#for idx in range(len(dataset)):\n",
    "#    data, applied_augmentors = dataset[idx]\n",
    "    # debug\n",
    "    #sample_idx = data[0]['frame_id']\n",
    "    #print(f\"{sample_idx}\")\n",
    "#    dataset_as_list.append(data)   \n",
    "    # dataset_as_list.append(dataset[idx])\n",
    "\n",
    "#save_data_list_zod(data_list=dataset_as_list, save_path=save_path, root_path=None, sample_id_list=sample_id_list, augmentors=applied_augmentors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "train_filename = '/home/rlab10/OpenPCDet/data/zod/zod_train_dataset.pkl'\n",
    "npy_filename = '/home/rlab10/OpenPCDet/data/zod/zod_train_dataset.npy'\n",
    "\n",
    "answer = input(\"Would you like to save the .pkl as .npy? (y/n): \")\n",
    "if answer.lower() == 'y':\n",
    "    with open(train_filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    np.save(npy_filename, data)\n",
    "    print(f\"Saved as {npy_filename}\")\n",
    "else:\n",
    "    print(\"Aborted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Beam-Labels with range image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pcdet.datasets.augmentor.data_augmentor import DataAugmentor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pkl_path = \"/home/rlab10/OpenPCDet/data/kitti/kitti_val_dataset_beamlabels.pkl\"\n",
    "\n",
    "with open(pkl_path, 'rb') as f:\n",
    "     data_list = pickle.load(f)\n",
    "    \n",
    "points = data_list[0]['points']\n",
    "beam_label = points[:, -1].astype(int)\n",
    "\n",
    "augmentor = DataAugmentor(root_path=None, augmentor_configs=[], class_names=[])\n",
    "polar_image = augmentor.get_polar_image(points[:, :3], with_limit_range=False)\n",
    "phi = polar_image[:, 0]\n",
    "theta = polar_image[:, 1]\n",
    "range = polar_image[:, 2] \n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sc = plt.scatter(phi, theta, c=range, cmap='jet', s=1)\n",
    "plt.xlabel('Azimuth (phi)')\n",
    "plt.ylabel('Elevation (theta)')\n",
    "plt.title('Range Image colored by Beam Label')\n",
    "plt.colorbar(sc, label='Beam Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug with MultiProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/microsoft/debugpy/issues/1168#issuecomment-1377998813\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.modules['debugpy'].__file__\n",
    "\n",
    "# go to '/home/rlab10/anaconda3/envs/pcdet/lib/python3.11/site-packages/debugpy\n",
    "# find ebugpy/server/api.py\n",
    "# change \"subProcess\": True to \"subProcess\": False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcdet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
