{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.kitti.kitti_dataset_custom import *\n",
    "from pcdet.datasets.dataset import *\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/kitti_dataset_custom.yaml')))\n",
    "class_names = ['Car', 'Pedestrian', 'Cyclist']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/kitti/kitti_dataset_custom.py' \n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'kitti' # raw data path\n",
    "save_path = ROOT_DIR / 'data' / 'kitti'\n",
    "kitti_infos = []\n",
    "num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "def create_kitti_infos(dataset_cfg, class_names, data_path, save_path, workers=4):\n",
    "    from time import sleep\n",
    "\n",
    "    dataset = KittiDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=False, logger=common_utils.create_logger())\n",
    "    \n",
    "    train_split, val_split, test_split = 'train', 'val', 'test'\n",
    "    num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "    train_filename = save_path / ('kitti_infos_%s.pkl' % train_split)\n",
    "    val_filename = save_path / ('kitti_%s_dataset.pkl' % val_split)\n",
    "    trainval_filename = save_path / ('kitti_infos_%s%s.pkl' % (train_split, val_split))\n",
    "    test_filename = save_path / ('kitti_infos_%s.pkl' % test_split)\n",
    "\n",
    "    print('\\n' + '-' * 36 + 'Start to generate data infos' + '-' * 37)\n",
    "    print('---------------CAUTION: Custom code is configured to serve as Augmentor NOT training-----------------')\n",
    "\n",
    "    dataset.set_split(train_split)\n",
    "    # ensure that get_infos() processes the single scene.\n",
    "    # NOTE: get_infos() collects infos about all classes (except 'DontCare'), filter unwanted classes with param `used_classes` in create_groundtruth_database.\n",
    "    kitti_infos_train = dataset.get_infos(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features)\n",
    "    with open(train_filename, 'wb') as f:\n",
    "        pickle.dump(kitti_infos_train, f)\n",
    "    print('Kitti info train file is saved to %s\\n' % train_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    dataset.set_split(val_split)\n",
    "    # ensure process single scene with PointFeatureEncoder, DataProcessor\n",
    "    dataset.training = False\n",
    "    allowed_classes = class_names\n",
    "    kitti_val_dataset = dataset.get_infos_val(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features, \n",
    "                                              class_names=allowed_classes, with_beam_label=dataset_cfg.WITH_BEAM_LABEL)\n",
    "    with open(val_filename, 'wb') as f:\n",
    "        pickle.dump(kitti_val_dataset, f)\n",
    "    print('Kitti info val dataset is saved to %s\\n' % val_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    with open(trainval_filename, 'wb') as f:\n",
    "        pickle.dump(kitti_infos_train + kitti_val_dataset, f)\n",
    "    print('Kitti info trainval file is saved to %s\\n' % trainval_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    dataset.set_split(test_split)\n",
    "    kitti_infos_test = dataset.get_infos(num_workers=workers, has_label=False, count_inside_pts=False)\n",
    "    with open(test_filename, 'wb') as f:\n",
    "       pickle.dump(kitti_infos_test, f)\n",
    "    print('Kitti info test file is saved to %s\\n' % test_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    print('\\n---------------Start creating groundtruth database for later data augmentation-------------------------')\n",
    "    print('---------------CAUTION: Custom code is configured to serve as Augmentor NOT training-------------------')\n",
    "    print('---------------No DataProcessor and PointFeatureEncoder required, handled by training data creation----')\n",
    "    \n",
    "    # Input the 'kitti_infos_train.pkl' to generate gt_database (cutted objects of samples)\n",
    "    dataset.set_split(train_split)\n",
    "    dataset.create_groundtruth_database(info_path=train_filename, used_classes=class_names, split=train_split, with_beam_labels=dataset_cfg.WITH_BEAM_LABEL)\n",
    "    print(f'---------------These groundtruth {train_split} objects are randomly inserted into samples (augmentation)-------')\n",
    "    print('-' * 41 + 'Data preparation Done' + '-' * 41)\n",
    "\n",
    "def save_data_list_kitti(data_list=None, save_path=None, root_path=None, sample_id_list=None, augmentors=None):\n",
    "\n",
    "    root_path = root_path if root_path is not None else Path(dataset_cfg.DATA_PATH) \n",
    "    split = dataset_cfg.DATA_SPLIT['train']\n",
    "    split_dir = root_path / 'ImageSets' / (split + '.txt')\n",
    "    #sample_id_list = [x.strip() for x in open(split_dir).readlines()] if split_dir.exists() else None\n",
    "    \n",
    "    train_split = 'train'\n",
    "    train_filename = save_path / ('kitti_%s_dataset.pkl' % train_split)\n",
    "\n",
    "    #aug_config_list = augmentors\n",
    "    #num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "    \n",
    "    print('\\n' + '-' * 35 + 'Start to save data infos(original+augmented)' + '-' * 37)\n",
    "    \n",
    "    with open(train_filename, 'wb') as f:\n",
    "        pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickler.dump(data_list)\n",
    "        pickler.clear_memo()\n",
    "\n",
    "    #for sample_idx in sample_id_list:\n",
    "    #    applied_augmentations = [str(name) for name in aug_config_list]\n",
    "    #    aug_str = ', '.join(applied_augmentations)\n",
    "    #    print(f\"{split} sample_idx: {sample_idx} (original, {aug_str})\")\n",
    "    \n",
    "    print('Kitti info train/aug file is saved to %s' % train_filename)\n",
    "    print('-' * 49 + 'Data saving Done' + '-' * 51 + '\\n') \n",
    "\n",
    "\n",
    "# Step 1 : Create the data_infos, only validation data_infos and gt_database are important. \n",
    "# The val data gets post-processed through DataProcessor, PointFeatureEncoder, also includes points (w FoV).\n",
    "# The gt_database is necessary for successfully creating augmented training samples.\n",
    "create_kitti_infos(dataset_cfg, class_names, data_path, save_path, workers=32)\n",
    "\n",
    "# Step 2: Create the training set with data augmentation\n",
    "dataset = KittiDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=True, logger=common_utils.create_logger()) # the training flag allows data augmentation before training\n",
    "\n",
    "# Step 3: Call the member method to catch information\n",
    "dataset.dataset_w_all_infos = dataset.get_infos(num_workers=32, has_label=True, count_inside_pts=True, num_features=num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save KittiDatasetCustom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Step 4: save it\n",
    "dataset_as_list = []\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    data, applied_augmentors = dataset[idx]\n",
    "    # debug\n",
    "    #sample_idx = data[0]['frame_id']\n",
    "    #print(f\"{sample_idx}\")\n",
    "    dataset_as_list.append(data)   \n",
    "    # dataset_as_list.append(dataset[idx])\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "save_data_list_kitti(data_list=dataset_as_list, save_path=save_path, root_path=None, sample_id_list=None, augmentors=applied_augmentors)\n",
    "\n",
    "# clean up variables after saving\n",
    "del dataset, dataset_as_list\n",
    "del data, applied_augmentors\n",
    "gc.collect()\n",
    "\n",
    "# clean up variables in notebook & # restart ipython kernel\n",
    "if get_ipython():\n",
    "    get_ipython().run_line_magic('reset', '-sf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom with range-based densification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.augmentor.data_augmentor import DataAugmentor\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/DG_KITTI/kitti_dataset_custom_densification.yaml')))\n",
    "class_names = ['Car', 'Pedestrian', 'Cyclist']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/kitti/kitti_dataset_custom.py'\n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'kitti' # raw data path\n",
    "save_path = ROOT_DIR / 'data' / 'kitti' / 'Domain Generalization' / 'densification'\n",
    "\n",
    "aug_cfg = dataset_cfg.DATA_AUGMENTOR['AUG_CONFIG_LIST'][0]\n",
    "name = aug_cfg.get('NAME')\n",
    "num_point_copies = aug_cfg.get('NUM_POINT_COPIES', 3)\n",
    "delta_r = aug_cfg.get('DELTA_R_RANGE', [0.05, 0.1])\n",
    "\n",
    "train_split, val_split = 'train', 'val'\n",
    "train_filename = data_path / ('kitti_%s_dataset.pkl' % train_split)\n",
    "dens_train_filename = save_path / ('kitti_%s_dataset_%sx_densified.pkl' % (train_split, num_point_copies))\n",
    "val_filename = data_path / ('kitti_%s_dataset.pkl' % val_split)\n",
    "dens_val_filename = save_path / ('kitti_%s_dataset_%sx_densified.pkl' % (val_split, num_point_copies))\n",
    "\n",
    "augmentor = DataAugmentor(root_path=data_path, augmentor_configs=dataset_cfg.DATA_AUGMENTOR, class_names=class_names, logger=common_utils.create_logger())\n",
    "if num_point_copies and delta_r and augmentor.logger is not None:\n",
    "    augmentor.logger.info('Range based densification enabled with Δr %s and num_copies %d' % (str(delta_r), num_point_copies))\n",
    "\n",
    "# training \n",
    "# with open(train_filename, 'rb') as f:\n",
    "#     data_list = pickle.load(f)\n",
    "\n",
    "# # Reduce point precision, bc .pkl is gettin really big\n",
    "# for sample in data_list:\n",
    "#     for data_dict in sample:\n",
    "#         if 'points' in data_dict:\n",
    "#             data_dict['points'] = data_dict['points'].astype('float16')\n",
    "\n",
    "# for sample in tqdm(data_list, desc=\"Samples\"):\n",
    "#     #print(f\"Processing frame_id: {sample[0].get('frame_id', 'N/A')}\")\n",
    "#     for i, data_dict in enumerate(sample):\n",
    "#         data_dict.pop('cam_info')\n",
    "#         data_dict.pop('lidar_aug_matrix')\n",
    "#         data_dict.pop('use_lead_xyz')\n",
    "#         data_dict.pop('flip_x', None)\n",
    "#         data_dict.pop('num_aug_beams', None)\n",
    "#         data_dict.pop('noise_world_rotation', None)\n",
    "#         data_dict.pop('noise_local_rotation', None)\n",
    "#         data_dict.pop('noise_local_scaling', None)\n",
    "#         data_dict.pop('noise_world_translation', None)\n",
    "#         for aug_func in augmentor.data_augmentor_queue:\n",
    "#             sample[i] = aug_func(data_dict)\n",
    "\n",
    "# with open(dens_train_filename, 'wb') as f:\n",
    "#         pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#         pickler.dump(data_list)\n",
    "#         print('Kitti info train dataset densified is saved to %s\\n' % dens_train_filename)\n",
    "#         pickler.clear_memo()\n",
    "\n",
    "# # empty RAM\n",
    "# del data_list\n",
    "# gc.collect()\n",
    "\n",
    "# validation\n",
    "with open(val_filename, 'rb') as f:\n",
    "     data_list = pickle.load(f)\n",
    "\n",
    "for sample in tqdm(data_list, desc=\"Samples\"):\n",
    "        for aug_func in augmentor.data_augmentor_queue:\n",
    "            sample = aug_func(sample)\n",
    "\n",
    "with open(dens_val_filename, 'wb') as f:\n",
    "        pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickler.dump(data_list)\n",
    "        print('Kitti info val dataset densified is saved to %s\\n' % dens_val_filename)\n",
    "        pickler.clear_memo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom with Random Beam Re-Sampling (RBRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.augmentor.data_augmentor import DataAugmentor\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/DG_KITTI/kitti_dataset_custom_rbrs.yaml')))\n",
    "class_names = ['Car', 'Pedestrian', 'Cyclist']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/kitti/kitti_dataset_custom.py'\n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'kitti'\n",
    "save_path = ROOT_DIR / 'data' / 'kitti' / 'Domain Generalization' / 'random beam re-sampling'\n",
    "\n",
    "train_split, val_split = 'train', 'val'\n",
    "train_filename = data_path / ('kitti_%s_dataset.pkl' % train_split)\n",
    "rbrs_train_filename = save_path / ('kitti_%s_dataset_rbrs.pkl' % train_split)\n",
    "val_filename = data_path / ('kitti_%s_dataset.pkl' % val_split)\n",
    "rbrs_val_filename = save_path / ('kitti_%s_dataset_rbrs.pkl' % val_split)\n",
    "\n",
    "aug_cfg = dataset_cfg.DATA_AUGMENTOR['AUG_CONFIG_LIST'][0]\n",
    "name = aug_cfg.get('NAME')\n",
    "upsampling_prob = aug_cfg.get('BEAM_UPSAMPLE_PROB', 1)\n",
    "phi = aug_cfg.get('PHI_THRESHOLD', 0.00301592894)\n",
    "range = aug_cfg.get('R_THRESHOLD', 2.0)\n",
    "num_interp_beams = aug_cfg.get('NUM_INTERP_BEAMS', 1)\n",
    "num_workers = 32\n",
    "\n",
    "augmentor = DataAugmentor(root_path=data_path, augmentor_configs=dataset_cfg.DATA_AUGMENTOR, class_names=class_names, logger=common_utils.create_logger())\n",
    "if phi and range and augmentor.logger is not None:\n",
    "    augmentor.logger.info('Random beam re-sampling enabled with upsample prob: %s, φ %s, r: %s and num. interp. beams: %s' % (upsampling_prob, phi, range, num_interp_beams))\n",
    "\n",
    "# train\n",
    "# with open(train_filename, 'rb') as f:\n",
    "#     data_list = pickle.load(f)\n",
    "\n",
    "# for sample in tqdm(data_list, desc=\"Samples\"):\n",
    "#     #print(f\"Processing frame_id: {sample[0].get('frame_id', 'N/A')}\")\n",
    "#     for i, data_dict in enumerate(sample):\n",
    "#         data_dict.pop('cam_info')\n",
    "#         data_dict.pop('lidar_aug_matrix')\n",
    "#         data_dict.pop('use_lead_xyz')\n",
    "#         data_dict.pop('flip_x', None)\n",
    "#         data_dict.pop('num_aug_beams', None)\n",
    "#         data_dict.pop('noise_world_rotation', None)\n",
    "#         data_dict.pop('noise_local_rotation', None)\n",
    "#         data_dict.pop('noise_local_scaling', None)\n",
    "#         data_dict.pop('noise_world_translation', None)\n",
    "#         for aug_func in augmentor.data_augmentor_queue:\n",
    "#             sample[i] = aug_func(data_dict)\n",
    "\n",
    "# with open(rbrs_train_filename, 'wb') as f:\n",
    "#         pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#         pickler.dump(data_list)\n",
    "#         print('Kitti info train dataset upsampled is saved to %s\\n' % rbrs_train_filename)\n",
    "#         pickler.clear_memo()\n",
    "\n",
    "# # empty RAM\n",
    "# del data_list\n",
    "# gc.collect()\n",
    "\n",
    "# validation\n",
    "with open(val_filename, 'rb') as f:\n",
    "     data_list = pickle.load(f)\n",
    "\n",
    "for sample in tqdm(data_list, desc=\"Samples\"):\n",
    "        for aug_func in augmentor.data_augmentor_queue:\n",
    "            sample = aug_func(sample)\n",
    "\n",
    "\n",
    "with open(rbrs_val_filename, 'wb') as f:\n",
    "        pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickler.dump(data_list)\n",
    "        print('Kitti info val dataset upsampled is saved to %s\\n' % rbrs_val_filename)\n",
    "        pickler.clear_memo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create KittiDatasetCustom with PDRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.augmentor.data_augmentor import DataAugmentor\n",
    "from pcdet.utils import common_utils\n",
    "from pcdet.datasets.augmentor import augmentor_utils\n",
    "\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import concurrent.futures as futures\n",
    "import time\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/DG_KITTI/kitti_dataset_custom_pdrw.yaml')))\n",
    "class_names = ['Car', 'Pedestrian', 'Cyclist']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/kitti/kitti_dataset_custom.py'\n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'kitti'\n",
    "save_path = ROOT_DIR / 'data' / 'kitti' / 'Domain Generalization' / 'pdrw interpolation'\n",
    "\n",
    "train_split, val_split = 'train', 'val'\n",
    "train_filename = data_path / ('kitti_%s_dataset.pkl' % train_split)\n",
    "pdrw_train_filename = save_path / ('kitti_%s_dataset_pdrw.pkl' % train_split)\n",
    "val_filename = data_path / ('kitti_%s_dataset.pkl' % val_split)\n",
    "pdrw_val_filename = save_path / ('kitti_%s_dataset_pdrw.pkl' % val_split)\n",
    "\n",
    "incl_file = data_path / 'training' / 'kitti scanning parameters' / 'incl.txt'\n",
    "height_file = data_path / 'training' / 'kitti scanning parameters' / 'height.txt'\n",
    "\n",
    "beam_layers = dataset_cfg.get('NUM_BEAMS', 64)\n",
    "range_image_width = dataset_cfg.get('WIDTH', 2048)\n",
    "max_range = dataset_cfg.get('MAX_RANGE', 60.0)\n",
    "\n",
    "incl = np.loadtxt(incl_file)\n",
    "height = np.loadtxt(height_file)\n",
    "num_workers = 32\n",
    "\n",
    "augmentor = DataAugmentor(root_path=data_path, augmentor_configs=[], class_names=class_names, logger=common_utils.create_logger())\n",
    "if beam_layers and range_image_width and augmentor.logger is not None:\n",
    "    augmentor.logger.info('Pixel-Distance and Range Weighted Interpolation with beams: %s, width rng_img: %s' % (beam_layers, range_image_width, ))\n",
    "\n",
    "# train\n",
    "# with open(train_filename, 'rb') as f:\n",
    "#      data_list = pickle.load(f)\n",
    "\n",
    "# def interpolate_sample_list(sample_list):\n",
    "#     for data_dict in sample_list:\n",
    "#         data_dict.pop('cam_info')\n",
    "#         data_dict.pop('lidar_aug_matrix')\n",
    "#         data_dict.pop('use_lead_xyz')\n",
    "#         data_dict.pop('flip_x', None)\n",
    "#         data_dict.pop('num_aug_beams', None)\n",
    "#         data_dict.pop('noise_world_rotation', None)\n",
    "#         data_dict.pop('noise_local_rotation', None)\n",
    "#         data_dict.pop('noise_local_scaling', None)\n",
    "#         data_dict.pop('noise_world_translation', None)\n",
    "#         points = data_dict['points']\n",
    "#         #beam_label = points[:, -1].astype(int)\n",
    "    \n",
    "#         range_image = augmentor_utils.get_range_image_hdl64e(\n",
    "#         points=points[:, :4],\n",
    "#         incl=incl,\n",
    "#         height=height\n",
    "#         )\n",
    "#         # range_image = augmentor_utils.get_range_image_hdl64e_beam_labels(points=points[:,:4],\n",
    "#         #                                                                 beam_labels=beam_label,\n",
    "#         #                                                                 num_beams=beam_layers,\n",
    "#         #                                                                 width=range_image_width)\n",
    "\n",
    "#         range_image_upsampled = augmentor.pixel_distance_range_weighted_interpolation(range_image, MAX_RANGE=max_range)\n",
    "#         points_interp = augmentor_utils.range_image_to_cartesian(range_image=range_image_upsampled, beam_label=True)\n",
    "\n",
    "#         data_dict['points'] = points_interp\n",
    "\n",
    "#     return sample_list\n",
    "\n",
    "# start_time = time.time()\n",
    "# with futures.ProcessPoolExecutor(num_workers) as executor:\n",
    "# # with futures.ThreadPoolExecutor(num_workers) as executor:\n",
    "#     data_list = list(tqdm(executor.map(interpolate_sample_list, data_list), total=len(data_list)))\n",
    "# end_time = time.time()\n",
    "# print(\"Total time for loading dataset: \", end_time - start_time, \"s\")\n",
    "# print(\"Loading speed for data: \", len(data_list) / (end_time - start_time), \"sample/s\")\n",
    "\n",
    "# with open(pdrw_train_filename, 'wb') as f:\n",
    "#         pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#         pickler.dump(data_list)\n",
    "#         print('Kitti info train dataset interpolated is saved to %s\\n' % pdrw_train_filename)\n",
    "#         pickler.clear_memo()\n",
    "\n",
    "# # empty RAM\n",
    "# del data_list\n",
    "# gc.collect()\n",
    "\n",
    "# validation\n",
    "with open(val_filename, 'rb') as f:\n",
    "     data_list = pickle.load(f)\n",
    "\n",
    "def interpolate_sample(sample):\n",
    "    points = sample['points']\n",
    "    #beam_label = points[:, -1].astype(int)\n",
    "    \n",
    "    range_image = augmentor_utils.get_range_image_hdl64e(\n",
    "       points=points[:, :4],\n",
    "       incl=incl,\n",
    "       height=height\n",
    "    )\n",
    "    # range_image = augmentor_utils.get_range_image_hdl64e_beam_labels(points=points[:,:4],\n",
    "    #                                                                 beam_labels=beam_label,\n",
    "    #                                                                 num_beams=beam_layers,\n",
    "    #                                                                 width=range_image_width)\n",
    "\n",
    "    range_image_upsampled = augmentor.pixel_distance_range_weighted_interpolation(range_image, MAX_RANGE=max_range)\n",
    "    points_interp = augmentor_utils.range_image_to_cartesian(range_image=range_image_upsampled, beam_label=True)\n",
    "\n",
    "    sample['points'] = points_interp\n",
    "\n",
    "    return sample\n",
    "\n",
    "start_time = time.time()\n",
    "with futures.ProcessPoolExecutor(num_workers) as executor:\n",
    "    data_list = list(tqdm(executor.map(interpolate_sample, data_list), total=len(data_list)))\n",
    "end_time = time.time()\n",
    "print(\"Total time for loading dataset: \", end_time - start_time, \"s\")\n",
    "print(\"Loading speed for data: \", len(data_list) / (end_time - start_time), \"sample/s\")\n",
    "\n",
    "with open(pdrw_val_filename, 'wb') as f:\n",
    "        pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickler.dump(data_list)\n",
    "        print('Kitti info val dataset interpolated is saved to %s\\n' % pdrw_val_filename)\n",
    "        pickler.clear_memo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ZODDatasetCustom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.zod.zod_dataset_custom import *\n",
    "from pcdet.datasets.dataset import *\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/zod_dataset_custom.yaml')))\n",
    "class_names = ['Vehicle_Car', 'Pedestrian', 'VulnerableVehicle_Bicycle']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/zod/zod_dataset_custom.py' \n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'zod'\n",
    "save_path = ROOT_DIR / 'data' / 'zod'\n",
    "zod_infos = []\n",
    "num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "def create_zod_infos(dataset_cfg, class_names, data_path, save_path, workers=4):\n",
    "    from time import sleep\n",
    "\n",
    "    dataset = ZODDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=False, logger=common_utils.create_logger(), creating_pkl_infos=True)\n",
    "    \n",
    "    train_split, val_split = 'train', 'val'\n",
    "    version = 'full'\n",
    "    num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "    train_filename = save_path / ('zod_infos_%s_%s.pkl' % (train_split, version))\n",
    "    val_filename = save_path / ('zod_%s_dataset.pkl' % val_split)\n",
    "    trainval_filename = save_path / ('zod_infos_trainval_%s.pkl' % version)\n",
    "\n",
    "    print('\\n' + '-' * 36 + 'Start to generate data infos' + '-' * 37)\n",
    "    print('---------------CAUTION: Custom code is configured to serve as Augmentor NOT training-----------------')\n",
    "\n",
    "    dataset.set_split(train_split, version)\n",
    "    zod_infos_train = dataset.get_infos(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features)\n",
    "    with open(train_filename, 'wb') as f:\n",
    "        pickle.dump(zod_infos_train, f)\n",
    "    print('Zod info train file is saved to %s\\n' % train_filename)\n",
    "    sleep(3) \n",
    "\n",
    "    dataset.set_split(val_split, version)\n",
    "    # ensure process single scene with PointFeatureEncoder, DataProcessor\n",
    "    dataset.training = False\n",
    "    allowed_classes = class_names\n",
    "    zod_val_dataset = dataset.get_infos_val(num_workers=workers, has_label=True, count_inside_pts=True, num_features=num_features, class_names=allowed_classes)\n",
    "    with open(val_filename, 'wb') as f:\n",
    "        pickle.dump(zod_val_dataset, f)\n",
    "    print('Zod info val file is saved to %s\\n' % val_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    with open(trainval_filename, 'wb') as f:\n",
    "        pickle.dump(zod_infos_train + zod_val_dataset, f)\n",
    "    print('Zod info trainval file is saved to %s\\n' % trainval_filename)\n",
    "    sleep(3)\n",
    "\n",
    "    print('\\n---------------Start creating groundtruth database for later data augmentation-------------------------')\n",
    "    print('---------------CAUTION: Custom code is configured to serve as Augmentor NOT training-------------------')\n",
    "    print('---------------No DataProcessor and PointFeatureEncoder required, handled by training data creation----')\n",
    "\n",
    "    # Input the 'zod_infos_train_full.pkl' to generate gt_database (cutted objects of samples)\n",
    "    dataset.set_split(train_split, version)\n",
    "    dataset.create_groundtruth_database(info_path=train_filename, version=version, split=train_split)\n",
    "    print(f'---------------These groundtruth {train_split} objects are randomly inserted into samples (augmentation)-------')\n",
    "    print('-' * 41 + 'Data preparation Done' + '-' * 41)\n",
    "\n",
    "def save_data_list_zod(data_list=None, save_path=None, root_path=None, sample_id_list=None, augmentors=None):\n",
    "    #import blosc\n",
    "    #import zstandard as zstd\n",
    "\n",
    "    root_path = root_path if root_path is not None else Path(dataset_cfg.DATA_PATH) \n",
    "    split = dataset_cfg.DATA_SPLIT['train']\n",
    "    \n",
    "    train_split = 'train'\n",
    "    train_filename = save_path / ('zod_%s_dataset.pkl' % train_split)\n",
    "\n",
    "    aug_config_list = augmentors\n",
    "    num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "    \n",
    "    print('\\n' + '-' * 35 + 'Start to save data infos(original+augmented)' + '-' * 37)\n",
    "    \n",
    "    # experimental\n",
    "    # raw = pickle.dumps(data_list, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # cctx = zstd.ZstdCompressor(level=15, threads=-1)\n",
    "    # comp = cctx.compress(raw)\n",
    "    # with open(train_filename, 'wb') as f:\n",
    "    #     f.write(comp)\n",
    "    \n",
    "    # experimental\n",
    "    #cctx = zstd.ZstdCompressor(level=3, threads=-1)\n",
    "    #with open(train_filename, 'wb') as fh, cctx.stream_writer(fh) as zfh:\n",
    "    #    pickler = pickle.Pickler(zfh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #    pickler.dump(data_list)\n",
    "    #    pickler.clear_memo()\n",
    "    \n",
    "    with open(train_filename, 'wb') as f:\n",
    "        pickler = pickle.Pickler(f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickler.dump(data_list)\n",
    "        pickler.clear_memo()\n",
    "    \n",
    "    # for sample_idx in sample_id_list:\n",
    "    #     applied_augmentations = [str(name) for name in aug_config_list]\n",
    "    #     aug_str = ', '.join(applied_augmentations)\n",
    "\n",
    "    #     print(f\"{split} sample_idx: {sample_idx} (original, {aug_str})\")\n",
    "    \n",
    "    print('Zod info train/aug file is saved to %s' % train_filename)\n",
    "    print('-' * 49 + 'Data saving Done' + '-' * 51 + '\\n') \n",
    "\n",
    "\n",
    "# Step 1 : Create the data_infos, only validatiosn data_infos and gt_database are important. \n",
    "# The val data gets post-processed through DataProcessor, PointFeatureEncoder, also includes points (w FoV).\n",
    "# The gt_database is necessary for successfully creating augmented training samples.\n",
    "create_zod_infos(dataset_cfg, class_names, data_path, save_path, workers=32)\n",
    "\n",
    "# Step 2: Create the training set with data augmentation\n",
    "dataset = ZODDatasetCustom(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=True, logger=common_utils.create_logger(), creating_pkl_infos=False)\n",
    "\n",
    "# Step 3: Call the member method to catch information\n",
    "train_split = 'train'\n",
    "version = 'full'\n",
    "train_filename = data_path / ('zod_infos_%s_%s.pkl' % (train_split, version))\n",
    "\n",
    "with open(train_filename, 'rb') as f:\n",
    "    zod_infos_train_full = pickle.load(f)\n",
    "\n",
    "sample_id_list = [info['point_cloud']['lidar_idx'] for info in zod_infos_train_full]\n",
    "\n",
    "dataset.dataset_w_all_infos = dataset.get_infos(num_workers=24, has_label=True, count_inside_pts=True, sample_id_list=sample_id_list, num_features=num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save ZODDatasetCustom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: save it (new) \n",
    "save_data_list_zod(\n",
    "    data_list=[dataset[i][0] for i in range(len(dataset.dataset_w_all_infos))], # dataset.dataset_w_all_infos\n",
    "    save_path=save_path,\n",
    "    root_path=None,\n",
    "    sample_id_list=sample_id_list,\n",
    "    augmentors=[cfg.get('NAME', str(cfg)) for cfg in dataset_cfg.DATA_AUGMENTOR.AUG_CONFIG_LIST],\n",
    ")\n",
    "\n",
    "# Step 4: save it (old, slower, OOM danger)\n",
    "#dataset_as_list = []\n",
    "\n",
    "#for idx in range(len(dataset)):\n",
    "#    data, applied_augmentors = dataset[idx]\n",
    "    # debug\n",
    "    #sample_idx = data[0]['frame_id']\n",
    "    #print(f\"{sample_idx}\")\n",
    "#    dataset_as_list.append(data)   \n",
    "    # dataset_as_list.append(dataset[idx])\n",
    "\n",
    "#save_data_list_zod(data_list=dataset_as_list, save_path=save_path, root_path=None, sample_id_list=sample_id_list, augmentors=applied_augmentors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f346b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "PKL_PATH = Path(\"/home/rlab10/OpenPCDet/data/zod/zod_val_dataset.pkl\")\n",
    "ZOD_FRAMES_ROOT = Path(\"/media/rlab10/Dataset/zod/single_frames\")\n",
    "\n",
    "def iter_frame_ids_from_pkl(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        fid = obj.get(\"frame_id\")\n",
    "        if fid is None:\n",
    "            fid = obj.get(\"point_cloud\", {}).get(\"lidar_idx\")\n",
    "        if fid is not None:\n",
    "            yield str(fid)\n",
    "        return\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        for x in obj:\n",
    "            yield from iter_frame_ids_from_pkl(x)\n",
    "\n",
    "def read_road_type_for_frame(frame_id: str):\n",
    "    meta_path = ZOD_FRAMES_ROOT / frame_id / \"metadata.json\"\n",
    "    if not meta_path.exists():\n",
    "        return frame_id, None, \"metadata_missing\"\n",
    "    try:\n",
    "        with meta_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "        return frame_id, meta.get(\"road_type\", None), None\n",
    "    except Exception as e:\n",
    "        return frame_id, None, f\"json_error: {type(e).__name__}\"\n",
    "\n",
    "def raw_key(val):\n",
    "    if val is None:\n",
    "        return \"<missing>\"\n",
    "    s = str(val).strip()\n",
    "    return s if s else \"<missing>\"\n",
    "\n",
    "with PKL_PATH.open(\"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "frame_ids = list(iter_frame_ids_from_pkl(data))\n",
    "frame_id_freq = Counter(frame_ids)              # zählt Duplikate im PKL\n",
    "frame_ids_unique = list(frame_id_freq.keys())   # jede metadata.json nur 1x lesen\n",
    "\n",
    "raw_counts = Counter()  # road_type_raw -> count (mit PKL-Duplikaten)\n",
    "errors = Counter()\n",
    "\n",
    "max_workers = 32\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "    futures = [ex.submit(read_road_type_for_frame, fid) for fid in frame_ids_unique]\n",
    "    for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Reading metadata.json\"):\n",
    "        fid, road_raw, err = fut.result()\n",
    "        mult = frame_id_freq[fid]\n",
    "        if err:\n",
    "            errors[err] += mult\n",
    "        raw_counts[raw_key(road_raw)] += mult\n",
    "\n",
    "print(f\"Total PKL frame_id occurrences (incl duplicates): {sum(frame_id_freq.values())}\")\n",
    "print(f\"Unique frames (metadata reads): {len(frame_ids_unique)}\")\n",
    "\n",
    "print(\"\\nAll unique road_type strings (raw):\")\n",
    "for s in sorted(raw_counts.keys()):\n",
    "    print(f\"  {s}\")\n",
    "\n",
    "print(\"\\nRaw road_type counts (incl duplicates):\")\n",
    "for s, c in raw_counts.most_common():\n",
    "    print(f\"  {s:25s} {c}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nMetadata read issues (top):\")\n",
    "    for k, v in errors.most_common(10):\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f20f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Config ---\n",
    "#PKL_PATH = Path(\"/home/rlab10/OpenPCDet/data/kitti/kitti_train_dataset.pkl\")\n",
    "PKL_PATH = Path(\"/home/rlab10/OpenPCDet/data/zod/zod_train_dataset.pkl\")\n",
    "\n",
    "LABEL_TO_NAME = {1: \"Car\", 2: \"Pedestrian\", 3: \"Cyclist\"}\n",
    "REQUIRED_LABELS = {1, 2, 3}   # must be present in ONE sample\n",
    "N_FIND = 5                    # how many matching samples to print\n",
    "MAX_SCAN = None               # e.g. 5000 to limit search, or None for all\n",
    "\n",
    "# Optional: if you already know an index and just want to inspect it\n",
    "SAMPLE_IDX = None             # e.g. 3400, or None to skip direct inspection\n",
    "\n",
    "\n",
    "# --- Helpers ---\n",
    "def get_original_dict(sample):\n",
    "    # Your pkl: each element is usually [original, aug1, aug2, ...]\n",
    "    if isinstance(sample, dict):\n",
    "        return sample\n",
    "    if isinstance(sample, (list, tuple)) and len(sample) > 0 and isinstance(sample[0], dict):\n",
    "        return sample[0]\n",
    "    raise TypeError(f\"Unexpected sample type/shape: {type(sample)}\")\n",
    "\n",
    "def extract_gt_boxes_lidar(d):\n",
    "    # Robust lookup across possible key layouts\n",
    "    boxes = d.get(\"gt_boxes_lidar\", None) or d.get(\"gt_boxes\", None)\n",
    "    if boxes is None and isinstance(d.get(\"annos\", None), dict):\n",
    "        boxes = d[\"annos\"].get(\"gt_boxes_lidar\", None) or d[\"annos\"].get(\"gt_boxes\", None)\n",
    "\n",
    "    if boxes is None:\n",
    "        return np.zeros((0, 0), dtype=np.float32)\n",
    "\n",
    "    arr = np.asarray(boxes)\n",
    "    if arr.size == 0:\n",
    "        return np.zeros((0, 0), dtype=np.float32)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr.reshape(1, -1)\n",
    "    return arr\n",
    "\n",
    "def get_frame_id(d0):\n",
    "    return d0.get(\"frame_id\", d0.get(\"point_cloud\", {}).get(\"lidar_idx\", \"<unknown>\"))\n",
    "\n",
    "def per_class_counts_from_labels(labels):\n",
    "    c = Counter(labels.astype(int, copy=False))\n",
    "    out = {LABEL_TO_NAME[k]: int(c.get(k, 0)) for k in sorted(REQUIRED_LABELS)}\n",
    "    unknown = {int(k): int(v) for k, v in c.items() if int(k) not in LABEL_TO_NAME}\n",
    "    return out, unknown\n",
    "\n",
    "\n",
    "# --- Load ---\n",
    "with PKL_PATH.open(\"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded PKL: {PKL_PATH}\")\n",
    "print(f\"Total samples in PKL: {len(data)}\")\n",
    "\n",
    "\n",
    "# --- Optional: inspect one chosen sample ---\n",
    "if SAMPLE_IDX is not None:\n",
    "    sample = data[SAMPLE_IDX]\n",
    "    d0 = get_original_dict(sample)\n",
    "    gt = extract_gt_boxes_lidar(d0)\n",
    "\n",
    "    print(\"\\n--- Inspect one sample ---\")\n",
    "    print(\"SAMPLE_IDX:\", SAMPLE_IDX)\n",
    "    print(\"frame_id:\", get_frame_id(d0))\n",
    "    print(\"gt shape:\", gt.shape)\n",
    "\n",
    "    if gt.shape[1] == 0:\n",
    "        print(\"No gt boxes found in this sample.\")\n",
    "    else:\n",
    "        labels = gt[:, -1]\n",
    "        counts, unknown = per_class_counts_from_labels(labels)\n",
    "        print(\"Present classes (counts):\", counts)\n",
    "        if unknown:\n",
    "            print(\"Unknown label IDs:\", unknown)\n",
    "\n",
    "\n",
    "# --- Search for samples that contain ALL 3 classes in original ---\n",
    "print(\"\\n--- Search samples with all 3 classes (original only) ---\")\n",
    "\n",
    "found = []\n",
    "data_iter = data if MAX_SCAN is None else data[:MAX_SCAN]\n",
    "\n",
    "for idx, sample in enumerate(tqdm(data_iter, desc=\"Scanning\")):\n",
    "    d0 = get_original_dict(sample)\n",
    "    gt = extract_gt_boxes_lidar(d0)\n",
    "\n",
    "    if gt.shape[1] == 0:\n",
    "        continue\n",
    "\n",
    "    labels = gt[:, -1].astype(int, copy=False)\n",
    "    present = set(labels.tolist())\n",
    "\n",
    "    if REQUIRED_LABELS.issubset(present):\n",
    "        counts, unknown = per_class_counts_from_labels(labels)\n",
    "        found.append((idx, get_frame_id(d0), counts, unknown))\n",
    "\n",
    "        if len(found) >= N_FIND:\n",
    "            break\n",
    "\n",
    "print(f\"Found {len(found)} matching samples.\")\n",
    "for idx, frame_id, counts, unknown in found:\n",
    "    print(f\"  SAMPLE_IDX={idx}  frame_id={frame_id}  counts={counts}\")\n",
    "    if unknown:\n",
    "        print(f\"    unknown_label_ids={unknown}\")\n",
    "\n",
    "if found:\n",
    "    best_idx = found[0][0]\n",
    "    print(f\"\\nTip: set SAMPLE_IDX = {best_idx} to inspect the first hit.\")\n",
    "else:\n",
    "    print(\"\\nNo sample found with all 3 classes within the scan range.\")\n",
    "    print(\"Try MAX_SCAN=None (full scan), or verify that label IDs are really {1,2,3} in the last gt column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Beam-Labels with Range Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pcdet.datasets.augmentor.data_augmentor import DataAugmentor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pkl_path = \"/home/rlab10/OpenPCDet/data/kitti/kitti_val_dataset_beamlabels.pkl\"\n",
    "\n",
    "with open(pkl_path, 'rb') as f:\n",
    "     data_list = pickle.load(f)\n",
    "    \n",
    "points = data_list[0]['points']\n",
    "beam_label = points[:, -1].astype(int)\n",
    "\n",
    "augmentor = DataAugmentor(root_path=None, augmentor_configs=[], class_names=[])\n",
    "polar_image = augmentor.get_polar_image(points[:, :3], with_limit_range=False)\n",
    "phi = polar_image[:, 0]\n",
    "theta = polar_image[:, 1]\n",
    "range = polar_image[:, 2] \n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sc = plt.scatter(phi, theta, c=range, cmap='jet', s=1)\n",
    "plt.xlabel('Azimuth (phi)')\n",
    "plt.ylabel('Elevation (theta)')\n",
    "plt.title('Range Image colored by Beam Label')\n",
    "plt.colorbar(sc, label='Beam Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug with MultiProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/microsoft/debugpy/issues/1168#issuecomment-1377998813\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.modules['debugpy'].__file__\n",
    "\n",
    "# go to '/home/rlab10/anaconda3/envs/pcdet/lib/python3.11/site-packages/debugpy\n",
    "# find ebugpy/server/api.py\n",
    "# change \"subProcess\": True to \"subProcess\": False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Range Image upsampled and 3D Point Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.datasets.augmentor.data_augmentor import DataAugmentor\n",
    "from pcdet.utils import common_utils\n",
    "from pcdet.datasets.augmentor import augmentor_utils\n",
    "\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dataset_cfg = EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/DG_KITTI/kitti_dataset_custom_pdrw.yaml')))\n",
    "class_names = ['Car', 'Pedestrian', 'Cyclist']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/kitti/kitti_dataset_custom.py'\n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'kitti'\n",
    "save_path = ROOT_DIR / 'data' / 'kitti' / 'Domain Generalization' / 'pdrw interpolation'\n",
    "\n",
    "val_split = 'val'\n",
    "val_filename = data_path / ('kitti_%s_dataset.pkl' % val_split)\n",
    "pdrw_val_filename = save_path / ('kitti_%s_dataset_pdrw.pkl' % val_split)\n",
    "\n",
    "# aug_cfg = dataset_cfg.DATA_AUGMENTOR['AUG_CONFIG_LIST'][0]\n",
    "# name = aug_cfg.get('NAME')\n",
    "# upsample_factor = aug_cfg.get('UPSAMPLE_FACTOR', 1)\n",
    "# sigma_d = aug_cfg.get('SIGMA_D', 0.5)\n",
    "# use_intensity = aug_cfg.get('USE_INTENSITY', True)\n",
    "\n",
    "# augmentor = DataAugmentor(root_path=data_path, augmentor_configs=dataset_cfg.DATA_AUGMENTOR, class_names=class_names, logger=common_utils.create_logger())\n",
    "# if upsample_factor and sigma_d and augmentor.logger is not None:\n",
    "#     augmentor.logger.info('Pixel-Distance and Range Weighted Interpolation with factor: %s, σ_d: %s, intensity: %s' % (upsample_factor, sigma_d, use_intensity))\n",
    "\n",
    "# validation\n",
    "with open(val_filename, 'rb') as f:\n",
    "     data_list = pickle.load(f)\n",
    "\n",
    "points = data_list[5]['points'] # (N, 5)\n",
    "lidar_idx = data_list[5]['point_cloud']['lidar_idx']\n",
    "print(lidar_idx)\n",
    "beam_label = points[:, -1].astype(int)\n",
    "#augmentor = DataAugmentor(root_path=data_path, augmentor_configs=dataset_cfg.DATA_AUGMENTOR, class_names=class_names, logger=common_utils.create_logger())\n",
    "augmentor = DataAugmentor(root_path=data_path, augmentor_configs=[], class_names=class_names, logger=common_utils.create_logger())\n",
    "# KITTI scanning parameters, obtained from Hough transformation\n",
    "height = np.loadtxt(\"/home/rlab10/OpenPCDet/data/kitti/training/kitti scanning parameters/height.txt\")\n",
    "zenith = np.loadtxt(\"/home/rlab10/OpenPCDet/data/kitti/training/kitti scanning parameters/zenith.txt\")\n",
    "incl = np.loadtxt(\"/home/rlab10/OpenPCDet/data/kitti/training/kitti scanning parameters/incl.txt\")\n",
    "\n",
    "# 1. Point Cloud -> Low-resolution 2D Range Image\n",
    "range_image = augmentor_utils.get_range_image_hdl64e(points[:, :4], incl=incl, height=height)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.imshow(range_image[:, :, 0], cmap='turbo')  # 0: Range, 1-3: x/y/z, 4: intensity\n",
    "plt.title('Range-Image (without calculated Beam-Labels)')\n",
    "plt.xlabel('Azimuth')\n",
    "plt.ylabel('Beam (row)')#\n",
    "plt.colorbar(label='Range [m]')\n",
    "plt.show()\n",
    "\n",
    "# Just an alternative to the function get_range_image_hdl64e()\n",
    "# range_image_2 = augmentor_utils.get_range_image_hdl64e_beam_labels(points=points[:, :4], beam_labels=beam_label, num_beams=64, width=2048)\n",
    "# plt.figure(figsize=(16, 6))\n",
    "# plt.imshow(range_image_2[:, :, 0], cmap='turbo')  # 0: Range, 1-3: x/y/z, 4: intensity\n",
    "# plt.title('Range-Image (with calculated Beam-Labels)')\n",
    "# plt.xlabel('Azimuth')\n",
    "# plt.ylabel('Beam (row)')\n",
    "# plt.colorbar(label='Range [m]')\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Empty pixels before:\", (range_image[:, :, 0] < 0).sum())\n",
    "# print(\"Empty pixels after Pixel-Distance Weighted Interpolation:\", (range_image_upsampled[:, :, 0] < 0).sum())\n",
    "\n",
    "# 4. Better Upsampling/Interpolation on Range-Image\n",
    "range_image_upsampled_ = augmentor.pixel_distance_range_weighted_interpolation(range_image, MAX_RANGE=60.0)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.imshow(range_image_upsampled_[:, :, 0], cmap='turbo')  # 0: Range, 1-3: x/y/z, 4: intensity\n",
    "plt.title('Range-Image (Upsampled Interpolation (advanced))')\n",
    "plt.xlabel('Azimuth')\n",
    "plt.ylabel('Beam (row)')\n",
    "plt.colorbar(label='Range [m]')\n",
    "plt.show()\n",
    "\n",
    "#print(\"Empty pixels before:\", (range_image_2[:, :, 0] < 0).sum())\n",
    "#print(\"Empty pixels after Pixel-Distance and Range Weighted Interpolation:\", (range_image_upsampled_[:, :, 0] < 0).sum())\n",
    "\n",
    "points_result = augmentor_utils.range_image_to_cartesian(range_image=range_image_upsampled_, beam_label=True)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(points_result[:, 0], points_result[:, 1], points_result[:, 2], \n",
    "           s=0.1, c=points_result[:, 2], cmap='jet')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('3D Point Cloud interpolated')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcdet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
