{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KITTI: Visualizer Raw Data or Pre-Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/rlab10/OpenPCDet')\n",
    "\n",
    "from pcdet.datasets.kitti.kitti_dataset_custom import *\n",
    "from pcdet.datasets.dataset import *\n",
    "from pcdet.datasets import DatasetTemplate\n",
    "from pcdet.utils import common_utils, calibration_kitti, box_utils\n",
    "\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Somehow without this my Open3D visualization didn't started.\n",
    "import os\n",
    "# CPU\n",
    "# os.environ[\"LIBGL_ALWAYS_INDIRECT\"]=\"0\"\n",
    "# os.environ[\"MESA_GL_VERSION_OVERRIDE\"]=\"4.5\"\n",
    "# os.environ[\"MESA_GLSL_VERSION_OVERRIDE\"]=\"450\"\n",
    "# os.environ[\"LIBGL_ALWAYS_SOFTWARE\"]=\"1\"\n",
    "\n",
    "# GPU\n",
    "# source: https://github.com/isl-org/Open3D/issues/5126#issuecomment-3374337980, https://github.com/PrismLauncher/PrismLauncher/issues/866#issuecomment-1432251741\n",
    "# https://github.com/isl-org/Open3D/issues/6872\n",
    "os.environ[\"XDG_SESSION_TYPE\"]=\"x11\"\n",
    "os.environ[\"GDK_BACKEND\"]=\"x11\"\n",
    "os.environ[\"MESA_D3D12_DEFAULT_ADAPTER_NAME\"]=\"NVIDIA\"\n",
    "\n",
    "from tools.visual_utils import open3d_vis_utils as Visualizer\n",
    "\n",
    "dataset_cfg=EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/kitti_dataset_custom.yaml')))\n",
    "class_names=['Car', 'Pedestrian', 'Cyclist']\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/kitti/kitti_dataset_custom.py' \n",
    "# /home/rlab10/OpenPCDet\n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'kitti'\n",
    "ext = '.bin'\n",
    "data = []\n",
    "num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "class DemoDatasetKitti(DatasetTemplate):\n",
    "    def __init__(self, dataset_cfg, class_names, training=True, root_path=None, logger=None, data_processor_flag=False, fov_mode=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_path:\n",
    "            dataset_cfg:\n",
    "            class_names:\n",
    "            training:\n",
    "            logger:\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            dataset_cfg=dataset_cfg, class_names=class_names, training=training, root_path=root_path, logger=logger\n",
    "        )\n",
    "        self.split = self.dataset_cfg.DATA_SPLIT['train']\n",
    "        self.root_split_path = self.root_path / ('training' if self.split != 'test' else 'testing')\n",
    "        split_dir = self.root_path / 'ImageSets' / (self.split + '.txt')\n",
    "        self.sample_id_list = [x.strip() for x in open(split_dir).readlines()] if split_dir.exists() else None\n",
    "        self.demo_kitti_infos = []\n",
    "        self.include_kitti_data(mode='train')\n",
    "        self.data_processor_flag = data_processor_flag\n",
    "        self.fov_mode = fov_mode\n",
    "\n",
    "    def include_kitti_data(self, mode):\n",
    "        if self.logger is not None:\n",
    "            self.logger.info('DemoDatasetKitti: Loading raw KITTI dataset')\n",
    "        demo_kitti_infos = []\n",
    "\n",
    "        for info_path in self.dataset_cfg.INFO_PATH[mode]: # 'train', bc training=True\n",
    "            info_path = self.root_path / info_path\n",
    "            if not info_path.exists():\n",
    "                continue\n",
    "            # Read the data infos from kitti_infos_train.pkl\n",
    "            with open(info_path, 'rb') as f:\n",
    "                infos = pickle.load(f)\n",
    "                demo_kitti_infos.extend(infos)\n",
    "        # Add the newly loaded KITTI dataset information to kitti_infos list.\n",
    "        self.demo_kitti_infos.extend(demo_kitti_infos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_id_list)\n",
    "    \n",
    "    def get_lidar(self, idx):\n",
    "        lidar_file = self.root_split_path / 'velodyne' / ('%s.bin' % idx)\n",
    "        assert lidar_file.exists()\n",
    "        return np.fromfile(str(lidar_file), dtype=np.float32).reshape(-1, 4)\n",
    "\n",
    "    def get_calib(self, idx):\n",
    "        calib_file = self.root_split_path / 'calib' / ('%s.txt' % idx)\n",
    "        assert calib_file.exists()\n",
    "        return calibration_kitti.Calibration(calib_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fov_flag(pts_rect, img_shape, calib):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pts_rect:\n",
    "            img_shape:\n",
    "            calib:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        pts_img, pts_rect_depth = calib.rect_to_img(pts_rect)\n",
    "        val_flag_1 = np.logical_and(pts_img[:, 0] >= 0, pts_img[:, 0] < img_shape[1])\n",
    "        val_flag_2 = np.logical_and(pts_img[:, 1] >= 0, pts_img[:, 1] < img_shape[0])\n",
    "        val_flag_merge = np.logical_and(val_flag_1, val_flag_2)\n",
    "        pts_valid_flag = np.logical_and(val_flag_merge, pts_rect_depth >= 0)\n",
    "\n",
    "        return pts_valid_flag\n",
    "\n",
    "    def prepare_demo_data(self, data_dict):\n",
    "        print('DemoDatasetKitti: prepare_demo_data() called')\n",
    "        if 'gt_boxes' not in data_dict:\n",
    "            assert 'gt_boxes' in data_dict, 'gt_boxes should be provided for demo visualization'\n",
    "        else:         \n",
    "            if data_dict.get('gt_boxes', None) is not None:\n",
    "                selected = common_utils.keep_arrays_by_name(data_dict['gt_names'], self.class_names)\n",
    "                data_dict['gt_boxes'] = data_dict['gt_boxes'][selected]\n",
    "                data_dict['gt_names'] = data_dict['gt_names'][selected]\n",
    "                gt_classes = np.array([self.class_names.index(n) + 1 for n in data_dict['gt_names']], dtype=np.int32)\n",
    "                # already transformed 3D boxes LiDAR coord. + add number for the class\n",
    "                gt_boxes = np.concatenate((data_dict['gt_boxes'], gt_classes.reshape(-1, 1).astype(np.float32)), axis=1)\n",
    "                data_dict['gt_boxes'] = gt_boxes\n",
    "\n",
    "\n",
    "            if data_dict.get('points', None) is not None:\n",
    "                data_dict = self.point_feature_encoder.forward(data_dict)\n",
    "            \n",
    "            if self.data_processor_flag:\n",
    "                print('DemoDatasetKitti: data processing activated')\n",
    "                data_dict = self.data_processor.forward(data_dict=data_dict)\n",
    "    \n",
    "        return data_dict\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print('DemoDatasetKitti: __getitem__ called')\n",
    "        if self.merge_all_iters_to_one_epoch:\n",
    "            index = index % len(self.demo_kitti_infos)\n",
    "        # works\n",
    "        info = copy.deepcopy(self.demo_kitti_infos[index])\n",
    "\n",
    "        sample_idx = info['point_cloud']['lidar_idx']\n",
    "        img_shape = info['image']['image_shape']\n",
    "        calib = self.get_calib(sample_idx)\n",
    "        get_item_list = self.dataset_cfg.get('GET_ITEM_LIST', ['points'])\n",
    "\n",
    "        input_dict = {\n",
    "            'frame_id': sample_idx,\n",
    "        }\n",
    "\n",
    "        if 'annos' in info:\n",
    "            annos = info['annos']\n",
    "            annos = common_utils.drop_info_with_name(annos, name = 'DontCare')\n",
    "            loc, dims, rots = annos['location'], annos['dimensions'], annos['rotation_y']\n",
    "            gt_names = annos['name']\n",
    "            gt_boxes_camera = np.concatenate([loc, dims, rots[...,np.newaxis]], axis = 1).astype(np.float32)\n",
    "            gt_boxes_lidar = box_utils.boxes3d_kitti_camera_to_lidar(gt_boxes_camera, calib)\n",
    "\n",
    "        input_dict.update({\n",
    "            'gt_names': gt_names,\n",
    "            'gt_boxes': gt_boxes_lidar\n",
    "        })\n",
    "\n",
    "        if 'points' in get_item_list:\n",
    "            points = self.get_lidar(sample_idx)\n",
    "\n",
    "            if self.fov_mode:\n",
    "                print('DemoDatasetKitti: fov mode activated')\n",
    "                # only the points in the FOV of the camera are important for me\n",
    "                pts_rect = calib.lidar_to_rect(points[:, 0:3])\n",
    "                fov_flag = self.get_fov_flag(pts_rect, img_shape, calib)\n",
    "                points = points[fov_flag]\n",
    "            input_dict['points'] = points\n",
    "\n",
    "        data_dict = self.prepare_demo_data(data_dict=input_dict)\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "def main(mode=str):\n",
    "    logger = common_utils.create_logger()\n",
    "    if mode == 'raw':\n",
    "        logger.info('-----------------Quick Visualizer Demo of raw data-------------------------')\n",
    "        logger.info(f'Mode for visualization: {mode}')\n",
    "        demo_dataset = DemoDatasetKitti(dataset_cfg=dataset_cfg, class_names=class_names, training=False, root_path=data_path, logger=logger, data_processor_flag=False, fov_mode=False)\n",
    "        logger.info(f'Total number of samples: \\t{len(demo_dataset)}')\n",
    "        # raw data not  0-4\n",
    "        #demo_dataset.demo_kitti_infos[4]\n",
    "        #demo_dataset[0]\n",
    "        # get_infos oder prepare_data hier implementieren(flow: include_kitti_data->__getitem__->prepare_data)\n",
    "        # or using KittiDataset for accessing the\n",
    "        data_dict = demo_dataset[4]\n",
    "        print('frame_id: ', data_dict['frame_id'])\n",
    "        points = data_dict['points']\n",
    "        gt_boxes = data_dict['gt_boxes']\n",
    "        gt_names = data_dict['gt_names']\n",
    "\n",
    "        Visualizer.draw_demo_scenes(points=points, gt_boxes=gt_boxes, gt_labels=gt_names, point_colors=None, draw_origin=True)\n",
    "        logger.info('Demo visualization of raw data done.')\n",
    "\n",
    "    elif mode == 'pre-processed':\n",
    "        logger.info('-----------------Quick Visualizer Demo of pre-processed data-------------------------')\n",
    "        logger.info(f'Mode for visualization: {mode}')\n",
    "        demo_dataset = DemoDatasetKitti(dataset_cfg=dataset_cfg, class_names=class_names, training=True, root_path=data_path, logger=logger, data_processor_flag=True, fov_mode=False)\n",
    "        logger.info(f'Total number of samples: \\t{len(demo_dataset)}')\n",
    "       \n",
    "        data_dict = demo_dataset[4]\n",
    "        print('frame_id: ', data_dict['frame_id'])\n",
    "        points = data_dict['points']\n",
    "        gt_boxes = data_dict['gt_boxes']\n",
    "        gt_names = data_dict['gt_names']\n",
    "\n",
    "        Visualizer.draw_demo_scenes(points=points, gt_boxes=gt_boxes, gt_labels=gt_names, point_colors=None, draw_origin=True)\n",
    "        logger.info('Demo visualization of pre-processed data done.')\n",
    "\n",
    "    elif mode == 'val':\n",
    "        logger.info('-----------------Quick Visualizer Demo of validation .pkl data-------------------------')\n",
    "        logger.info(f'Mode for visualization: {mode}')\n",
    "        #pkl_path = ROOT_DIR / 'data' / 'kitti' / 'data_test_pipeline' / 'visualizer' / 'kitti_val_dataset.pkl'\n",
    "        pkl_path = ROOT_DIR / 'data' / 'kitti' / 'kitti_val_dataset.pkl'\n",
    "        #pkl_path = ROOT_DIR / 'data' / 'kitti' / 'Domain Generalization' / 'densification' / 'kitti_val_dataset_3x_densified.pkl'\n",
    "        #pkl_path = ROOT_DIR / 'data' / 'kitti' / 'Domain Generalization' / 'random beam re-sampling' / 'kitti_val_dataset_rbrs.pkl'\n",
    "        #pkl_path = ROOT_DIR / 'data' / 'kitti' / 'Domain Generalization' / 'pdrw interpolation' / 'kitti_val_dataset_pdrw.pkl'\n",
    "        \n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            demo_val_dataset = pickle.load(f)\n",
    "\n",
    "        data_dict = demo_val_dataset[4]\n",
    "        points = data_dict['points']\n",
    "        gt_boxes_lidar = data_dict['annos']['gt_boxes_lidar']\n",
    "        gt_names = data_dict['annos']['name']\n",
    "        gt_scores = data_dict['annos']['score']\n",
    "        lidar_idx = data_dict['point_cloud']['lidar_idx']\n",
    "        print(lidar_idx)\n",
    "\n",
    "        #class_n = data_dict['annos']['name']\n",
    "        #print(f\"\\nLidar IDX: {lidar_idx}\")\n",
    "        #print(f\"Points: {points}\\n\")\n",
    "        #print(f\"Class Names: {class_n}\\n\")\n",
    "        #print(f\"Ground Truth Boxes (LiDAR): {gt_boxes_lidar}\")\n",
    "\n",
    "        Visualizer.draw_demo_scenes(points=points, gt_boxes=gt_boxes_lidar, gt_labels=gt_names, gt_score=gt_scores, point_colors=None, draw_origin=True, vc='val')\n",
    "        logger.info('Demo visualization of pre-processed validation data done.')\n",
    "\n",
    "    elif mode == 'train':\n",
    "        logger.info('-----------------Quick Visualizer Demo of training .pkl data-------------------------')\n",
    "        logger.info(f'Mode for visualization: {mode}')\n",
    "        #pkl_path = ROOT_DIR / 'data' / 'kitti' / 'data_test_pipeline' / 'visualizer' / 'kitti_train_dataset.pkl'\n",
    "        pkl_path = ROOT_DIR / 'data' / 'kitti' / 'kitti_train_dataset.pkl'\n",
    "        #pkl_path = ROOT_DIR / 'data' / 'kitti' / 'Domain Generalization' / 'densification' / 'kitti_train_dataset_3x_densified.pkl'\n",
    "        #pkl_path = ROOT_DIR / 'data' / 'kitti' / 'Domain Generalization' / 'random beam re-sampling' / 'kitti_train_dataset_rbrs.pkl'\n",
    "        #pkl_path = ROOT_DIR / 'data' / 'kitti' / 'Domain Generalization' / 'pdrw interpolation' / 'kitti_train_dataset_pdrw.pkl' # points_interp\n",
    "    \n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            demo_train_dataset = pickle.load(f)\n",
    "\n",
    "        data_list = demo_train_dataset[4] # specific sample from row no. in train.txt, e.g. (34-1) - 000067)\n",
    "        # [0] - original; [1] - gt_sampling; [2] - flip_x; [3] - w_rotation; [4] - l_rotation; \n",
    "        # [5] - l_scaling; [6] - w_translation\n",
    "        try:\n",
    "            print(\"\\nPlease select an option for the visualization:\")\n",
    "            print('[0] Original')\n",
    "            print('[1] Ground Truth Sampling (gt_sampling)')\n",
    "            print('[2] Flip along X-axis (flip_x)')\n",
    "            print('[3] World rotation (w_rotation)')\n",
    "            print('[4] Local rotation (l_rotation)')\n",
    "            print('[5] Local scaling (l_scaling)')\n",
    "            print('[6] World translation (w_translation)\\n')\n",
    "            user_choice = int(input('Your selection: '))\n",
    "            if user_choice not in range(7):\n",
    "                raise ValueError('Invalid selection')\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}. Please enter a number between 0 and 6.\")\n",
    "            logger.error(\"Invalid user input for the visualization mode.\")\n",
    "            return\n",
    "        \n",
    "        switch_cases = {0: \"original\", 1: \"gt_sampling\", 2: \"flip_x\", 3: \"w_rotation\", 4: \"l_rotation\", 5: \"l_scaling\", 6: \"w_translation\"}\n",
    "        \n",
    "        selected_data = data_list[user_choice]\n",
    "        points = selected_data['points'] \n",
    "        gt_boxes = selected_data['gt_boxes']\n",
    "        gt_names = selected_data['gt_names']\n",
    "        gt_scores = selected_data['cam_info']['score'] # take it from annotations (Camera frame)\n",
    "        frame_id = selected_data['frame_id']\n",
    "        print(frame_id)\n",
    "\n",
    "        if user_choice in [3, 4]: \n",
    "            noise_key = 'noise_world_rotation' if user_choice == 3 else 'noise_local_rotation'\n",
    "            noise_rot = selected_data.get(noise_key, None)\n",
    "            if noise_rot is not None:\n",
    "                logger.info(f'Noise rotation for {switch_cases[user_choice]}: {noise_rot}')\n",
    "        if user_choice == 5:\n",
    "            noise_scale = selected_data.get('noise_local_scaling', None)\n",
    "            if noise_scale is not None:\n",
    "                logger.info(f'Noise scaling for {switch_cases[user_choice]}: {noise_scale}')\n",
    "        if user_choice == 6:\n",
    "            noise_trans = selected_data.get('noise_world_translation', None)\n",
    "            if noise_trans is not None:\n",
    "                logger.info(f'Noise translation for {switch_cases[user_choice]}: {noise_trans}')\n",
    "        \n",
    "\n",
    "        Visualizer.draw_demo_scenes(points=points, gt_boxes=gt_boxes, gt_labels=gt_names, gt_score=gt_scores, point_colors=None, draw_origin=True, vc='')\n",
    "        logger.info('Demo visualization of training data done.')\n",
    "    \n",
    "    if get_ipython():\n",
    "        get_ipython().run_line_magic('reset', '-sf')\n",
    "        #get_ipython().kernel.do_shutdown(restart=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #main(mode='raw')\n",
    "    #main(mode='pre-processed')\n",
    "    #main(mode='val')\n",
    "    main(mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZOD: Visualizer Raw Data or Pre-Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 08:38:47,500   INFO  -----------------Quick Visualizer Demo of validation .pkl data-------------------------\n",
      "2026-02-17 08:38:47,500   INFO  -----------------Quick Visualizer Demo of validation .pkl data-------------------------\n",
      "2026-02-17 08:38:47,503   INFO  Mode for visualization: val\n",
      "2026-02-17 08:38:47,503   INFO  Mode for visualization: val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 08:38:58,877   INFO  Demo visualization of pre-processed validation data done.\n",
      "2026-02-17 08:38:58,877   INFO  Demo visualization of pre-processed validation data done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/rlab10/OpenPCDet')\n",
    "\n",
    "from pcdet.datasets.zod.zod_dataset_custom import *\n",
    "from pcdet.datasets.dataset import *\n",
    "from pcdet.datasets import DatasetTemplate\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "from zod import ZodFrames\n",
    "from zod.constants import Camera, Lidar\n",
    "from zod.utils.geometry import get_points_in_camera_fov, transform_points\n",
    "from zod.data_classes.geometry import Pose\n",
    "\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Somehow without this my Open3D visualization didn't started.\n",
    "import os\n",
    "# CPU\n",
    "#os.environ[\"LIBGL_ALWAYS_INDIRECT\"]=\"0\"\n",
    "#os.environ[\"MESA_GL_VERSION_OVERRIDE\"]=\"4.5\"\n",
    "#os.environ[\"MESA_GLSL_VERSION_OVERRIDE\"]=\"450\"\n",
    "#os.environ[\"LIBGL_ALWAYS_SOFTWARE\"]=\"1\"\n",
    "\n",
    "# GPU\n",
    "# source: https://github.com/isl-org/Open3D/issues/5126#issuecomment-3374337980, https://github.com/PrismLauncher/PrismLauncher/issues/866#issuecomment-1432251741\n",
    "# https://github.com/isl-org/Open3D/issues/6872\n",
    "os.environ[\"XDG_SESSION_TYPE\"]=\"x11\"\n",
    "os.environ[\"GDK_BACKEND\"]=\"x11\"\n",
    "os.environ[\"MESA_D3D12_DEFAULT_ADAPTER_NAME\"]=\"NVIDIA\"\n",
    "\n",
    "from tools.visual_utils import open3d_vis_utils as Visualizer\n",
    "\n",
    "dataset_cfg=EasyDict(yaml.safe_load(open('/home/rlab10/OpenPCDet/tools/cfgs/dataset_configs/zod_dataset_custom.yaml')))\n",
    "class_names = ['Car', 'Pedestrian', 'Cyclist'] # use KITTI classes, bc we map with map_merged_classes()\n",
    "file_path = '/home/rlab10/OpenPCDet/pcdet/datasets/zod/zod_dataset_custom.py' \n",
    "# /home/rlab10/OpenPCDet\n",
    "ROOT_DIR = (Path(file_path).resolve().parent / '../../../').resolve()\n",
    "data_path = ROOT_DIR / 'data' / 'zod'\n",
    "ext = '.npy'\n",
    "data = []\n",
    "num_features = len(dataset_cfg.POINT_FEATURE_ENCODING.src_feature_list)\n",
    "\n",
    "class DemoDatasetZod(DatasetTemplate):\n",
    "    def __init__(self, dataset_cfg, class_names, training=True, root_path=None, logger=None, data_processor_flag=False, fov_mode=False, creating_pkl_infos=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_path:\n",
    "            dataset_cfg:\n",
    "            class_names:\n",
    "            training:\n",
    "            logger:\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            dataset_cfg=dataset_cfg, class_names=class_names, training=training, root_path=root_path, logger=logger\n",
    "        )\n",
    "\n",
    "        self.split = self.dataset_cfg.DATA_SPLIT[self.mode]\n",
    "        self.version = self.dataset_cfg.DATASET_VERSION \n",
    "        self.countries =  self.dataset_cfg.DATASET_COUNTRIES\n",
    "        self.creating_pkl_infos = creating_pkl_infos\n",
    "        self.zod_frames = ZodFrames(dataset_root=self.root_path, version=self.version)\n",
    "\n",
    "        split_dir = self.root_path / 'ImageSets' / (self.split + '_' + self.version + '.txt') # ZOD\n",
    "        self.sample_id_list = [x.strip() for x in open(split_dir).readlines()] if split_dir.exists() else None\n",
    "\n",
    "        self.demo_zod_infos = []\n",
    "        self.data_processor_flag = data_processor_flag\n",
    "        self.fov_mode = fov_mode\n",
    "\n",
    "        self.Tr_Zod_Lidar_to_Kitti_Lidar = np.array([[0, -1, 0],\n",
    "                                                  [1,  0, 0], \n",
    "                                                  [0,  0, 1]])\n",
    "\n",
    "        self.include_zod_data(mode='train')\n",
    "\n",
    "    def include_zod_data(self, mode):\n",
    "        if self.logger is not None:\n",
    "            self.logger.info('DemoDatasetZod: Loading raw ZOD dataset')\n",
    "        demo_zod_infos = []\n",
    "\n",
    "        for info_path in self.dataset_cfg.INFO_PATH[mode]: # 'train', bc training=True\n",
    "            info_path = self.root_path / info_path\n",
    "            if not info_path.exists():\n",
    "                continue\n",
    "            # Read the data infos from zod_infos_train.pkl\n",
    "            with open(info_path, 'rb') as f:\n",
    "                infos = pickle.load(f)\n",
    "                demo_zod_infos.extend(infos)\n",
    "\n",
    "        self.demo_zod_infos.extend(demo_zod_infos)\n",
    "        if self.logger is not None:\n",
    "            self.logger.info('Total samples for ZOD dataset: %d' % (len(demo_zod_infos)))\n",
    "\n",
    "        if not self.creating_pkl_infos:\n",
    "            self. map_merged_classes()\n",
    "        \n",
    "    def map_merged_classes(self):\n",
    "        if self.dataset_cfg.get('MAP_MERGED_CLASSES', None) is None:\n",
    "            return\n",
    "        \n",
    "        #update class names in zod_infos\n",
    "        map_merge_class = self.dataset_cfg.MAP_MERGED_CLASSES\n",
    "        for info in self.demo_zod_infos:\n",
    "            assert 'annos' in info\n",
    "            info['annos']['name'] = np.vectorize(lambda name: map_merge_class[name], otypes=[str])(info['annos']['name'])\n",
    "\n",
    "    def __len__(self):\n",
    "        if self._merge_all_iters_to_one_epoch:\n",
    "            if self.logger is not None:\n",
    "                self.logger.info('zod_infos: %s', self.demo_zod_infos)\n",
    "            return len(self.demo_zod_infos) * self.total_epochs \n",
    "\n",
    "        return len(self.demo_zod_infos)\n",
    "    \n",
    "    def get_lidar(self, idx, num_features=4):\n",
    "        \"\"\"\n",
    "            Loads point cloud for a sample\n",
    "                Args: \n",
    "                    index (int): Index of the point cloud file to get.\n",
    "                Returns:\n",
    "                    np.array(N, 4): point cloud\n",
    "        \"\"\"\n",
    "        try:\n",
    "            zod_frames_files = self.zod_frames[idx]\n",
    "            lidar_core_frame = zod_frames_files.info.get_key_lidar_frame()\n",
    "            pc = lidar_core_frame.read()\n",
    "\n",
    "            if self.dataset_cfg.get('USE_VLS128_ONLY', False): \n",
    "                vls128_mask = pc.diode_idx < 128\n",
    "                pc.points = pc.points[vls128_mask]\n",
    "                pc.intensity = pc.intensity[vls128_mask]\n",
    "                pc.diode_idx = pc.diode_idx[vls128_mask]\n",
    "                pc.timestamps = pc.timestamps[vls128_mask]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Lidar for {idx}: {e}\")\n",
    "\n",
    "        if num_features == 4:\n",
    "            # scale intensity to [0,1] from [0,255], bc at ZOD it isn't default\n",
    "            pc.intensity = pc.intensity / 255\n",
    "            # (x, y, z, intensity)\n",
    "            points = np.concatenate((pc.points, pc.intensity.reshape(-1,1)), dtype=np.float32, axis=1)\n",
    "        elif num_features == 5:\n",
    "            pc.intensity = pc.intensity / 255\n",
    "            points = np.concatenate((pc.points, pc.intensity.reshape(-1, 1), pc.diode_idx.reshape(-1, 1)), dtype=np.float32, axis=1)\n",
    "        elif num_features == 3:\n",
    "            points = pc.points\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return points\n",
    "    \n",
    "    def get_calib(self, idx):\n",
    "        zod_frame = self.zod_frames[idx]\n",
    "\n",
    "        return zod_frame.calibration\n",
    "\n",
    "    def get_fov_flag(self, pts_lidar, calib, camera=Camera.FRONT, lidar=Lidar.VELODYNE, use_kitti_fov=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points (np.ndarray): LiDAR points in ZOD LiDAR coordinate system, shape (N, 3)\n",
    "            calib: ZOD calibration object with camera and LiDAR extrinsics\n",
    "            camera: ZOD Camera Enum (default: FRONT)\n",
    "            lidar: ZOD Lidar Enum (default: VELODYNE)\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Boolean mask (N,), True for point in FoV of camera\n",
    "        \"\"\"\n",
    "        # Transformation LiDAR -> World\n",
    "        t_lidar_to_world = calib.lidars[lidar].extrinsics\n",
    "        # Transformation Camera -> World\n",
    "        t_camera_to_world = calib.cameras[camera].extrinsics\n",
    "\n",
    "        # Transformation World -> Camera\n",
    "        t_world_to_camera= t_camera_to_world.inverse\n",
    "        # Combine transformations LiDAR -> World -> Camera\n",
    "        t_lidar_to_camera = Pose(t_world_to_camera.transform @ t_lidar_to_world.transform)\n",
    "\n",
    "        #points_img = transform_points(pts_lidar[:, :3], t_lidar_to_camera.transform)\n",
    "        points_img = transform_points(pts_lidar, t_lidar_to_camera.transform)\n",
    "\n",
    "        # Only points with positive\n",
    "        positive_depth = points_img[:, 2] > 0 # z>0\n",
    "        mask = np.zeros(pts_lidar.shape[0], dtype=bool)\n",
    "        if not np.any(positive_depth):\n",
    "            return mask\n",
    "\n",
    "        points_img_valid = points_img[positive_depth]\n",
    "\n",
    "        if use_kitti_fov:\n",
    "            kitti_fov = self.dataset_cfg.KITTI_FOV # (90째, 35째)\n",
    "            _, fov_mask = get_points_in_camera_fov(fov=kitti_fov, camera_data=points_img_valid) # KITTI\n",
    "        else:\n",
    "            zod_fov = calib.cameras[camera].field_of_view # (120째, 67째)\n",
    "            _, fov_mask = get_points_in_camera_fov(fov=zod_fov, camera_data=points_img_valid) # ZOD\n",
    "\n",
    "        mask[positive_depth] = fov_mask\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def prepare_demo_data(self, data_dict):\n",
    "        print('DemoDatasetZod: prepare_demo_data() called')\n",
    "        if 'gt_boxes' not in data_dict:\n",
    "            assert 'gt_boxes' in data_dict, 'gt_boxes should be provided for demo visualization'\n",
    "        else:         \n",
    "            if data_dict.get('gt_boxes', None) is not None:\n",
    "                selected = common_utils.keep_arrays_by_name(data_dict['gt_names'], self.class_names)\n",
    "                data_dict['gt_boxes'] = data_dict['gt_boxes'][selected]\n",
    "                data_dict['gt_names'] = data_dict['gt_names'][selected]\n",
    "                gt_classes = np.array([self.class_names.index(n) + 1 for n in data_dict['gt_names']], dtype=np.int32)\n",
    "                # already transformed 3D boxes LiDAR coord. + add number for the class\n",
    "                gt_boxes = np.concatenate((data_dict['gt_boxes'], gt_classes.reshape(-1, 1).astype(np.float32)), axis=1)\n",
    "                data_dict['gt_boxes'] = gt_boxes\n",
    "\n",
    "            if data_dict.get('points', None) is not None:\n",
    "                data_dict = self.point_feature_encoder.forward(data_dict)\n",
    "            \n",
    "            if self.data_processor_flag:\n",
    "                print('DemoDatasetZod: data processing activated')\n",
    "                data_dict = self.data_processor.forward(data_dict=data_dict)\n",
    "    \n",
    "        return data_dict\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print('DemoDatasetZod: __getitem__ called')\n",
    "        if self.merge_all_iters_to_one_epoch:\n",
    "            index = index % len(self.demo_zod_infos)\n",
    "        # works\n",
    "        info = copy.deepcopy(self.demo_zod_infos[index])\n",
    "\n",
    "        sample_idx = info['point_cloud']['lidar_idx']\n",
    "        calib = self.get_calib(sample_idx)\n",
    "        get_item_list = self.dataset_cfg.get('GET_ITEM_LIST', ['points'])\n",
    "\n",
    "        input_dict = {\n",
    "            'frame_id': sample_idx,\n",
    "        }\n",
    "\n",
    "        if 'annos' in info:\n",
    "            annos = info['annos']\n",
    "            annos = common_utils.drop_info_with_name(annos, name = 'DontCare')\n",
    "            gt_names = annos['name']\n",
    "            gt_boxes_lidar = annos['gt_boxes_lidar']\n",
    "\n",
    "        input_dict.update({\n",
    "            'gt_names': gt_names,\n",
    "            'gt_boxes': gt_boxes_lidar\n",
    "        })\n",
    "\n",
    "        if 'points' in get_item_list:\n",
    "            points = self.get_lidar(sample_idx, num_features=4)\n",
    "\n",
    "            if self.fov_mode:\n",
    "                print('DemoDatasetZod: fov mode activated')\n",
    "                # only the points in the FOV of the camera are important for me\n",
    "                fov_flag = self.get_fov_flag(points[:, 0:3], calib)\n",
    "                points = points[fov_flag]\n",
    "            \n",
    "            points[:, :3] = points[:, :3] @ self.Tr_Zod_Lidar_to_Kitti_Lidar\n",
    "            points[:, 2] -= self.dataset_cfg.LIDAR_Z_SHIFT\n",
    "\n",
    "            input_dict['points'] = points\n",
    "\n",
    "        data_dict = self.prepare_demo_data(data_dict=input_dict)\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "def main(mode=str):\n",
    "    logger = common_utils.create_logger()\n",
    "    if mode == 'raw':\n",
    "        logger.info('-----------------Quick Visualizer Demo of raw data-------------------------')\n",
    "        logger.info(f'Mode for visualization: {mode}')\n",
    "        demo_dataset = DemoDatasetZod(dataset_cfg=dataset_cfg, class_names=class_names, training=False, root_path=data_path, logger=logger, data_processor_flag=False, fov_mode=False, creating_pkl_infos=False)\n",
    "        logger.info(f'Total number of samples: \\t{len(demo_dataset)}')\n",
    "      \n",
    "        data_dict = demo_dataset[0] # 0 - 3711\n",
    "        print('frame_id: ', data_dict['frame_id'])\n",
    "        points = data_dict['points']\n",
    "        gt_boxes = data_dict['gt_boxes']\n",
    "        gt_names = data_dict['gt_names']\n",
    "\n",
    "        Visualizer.draw_demo_scenes(points=points, gt_boxes=gt_boxes, gt_labels=gt_names, point_colors=None, draw_origin=True)\n",
    "        logger.info('Demo visualization of raw data done.')\n",
    "\n",
    "    elif mode == 'pre-processed':\n",
    "        logger.info('-----------------Quick Visualizer Demo of pre-processed data-------------------------')\n",
    "        logger.info(f'Mode for visualization: {mode}')\n",
    "        demo_dataset = DemoDatasetZod(dataset_cfg=dataset_cfg, class_names=class_names, training=False, root_path=data_path, logger=logger, data_processor_flag=True, fov_mode=True, creating_pkl_infos=False)\n",
    "        logger.info(f'Total number of samples: \\t{len(demo_dataset)}')\n",
    "       \n",
    "        data_dict = demo_dataset[0] # 0 - 3711\n",
    "        print('frame_id: ', data_dict['frame_id'])\n",
    "        points = data_dict['points']\n",
    "        gt_boxes = data_dict['gt_boxes']\n",
    "        gt_names = data_dict['gt_names']\n",
    "\n",
    "        Visualizer.draw_demo_scenes(points=points, gt_boxes=gt_boxes, gt_labels=gt_names, point_colors=None, draw_origin=True)\n",
    "        logger.info('Demo visualization of pre-processed data done.')\n",
    "\n",
    "    elif mode == 'val':\n",
    "\n",
    "        logger.info('-----------------Quick Visualizer Demo of validation .pkl data-------------------------')\n",
    "        logger.info(f'Mode for visualization: {mode}')\n",
    "        pkl_path = ROOT_DIR / 'data' / 'zod' / 'zod_val_dataset.pkl'\n",
    "\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            demo_val_dataset = pickle.load(f)\n",
    "\n",
    "        data_dict = demo_val_dataset[3582] # 0 - 3768\n",
    "        points = data_dict['points']\n",
    "        gt_boxes_lidar = data_dict['annos']['gt_boxes_lidar']\n",
    "        gt_names = data_dict['annos']['name']\n",
    "        gt_scores = data_dict['annos']['score']\n",
    "        lidar_idx = data_dict['point_cloud']['lidar_idx']\n",
    "        print(lidar_idx)\n",
    "\n",
    "        Visualizer.draw_demo_scenes(points=points, gt_boxes=gt_boxes_lidar, gt_labels=gt_names, gt_score=gt_scores, point_colors=None, draw_origin=True, vc='val')\n",
    "        logger.info('Demo visualization of pre-processed validation data done.')\n",
    "\n",
    "    elif mode == 'train':\n",
    "\n",
    "        logger.info('-----------------Quick Visualizer Demo of training .pkl data-------------------------')\n",
    "        logger.info(f'Mode for visualization: {mode}')\n",
    "        pkl_path = ROOT_DIR / 'data' / 'zod' / 'zod_train_dataset.pkl'\n",
    "\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            demo_train_dataset = pickle.load(f)\n",
    "\n",
    "        data_list = demo_train_dataset[4] # specific sample from row no. in train.txt, e.g. (34-1) - 000067)\n",
    "        # [0] - original; [1] - gt_sampling; [2] - flip_x; [3] - w_rotation; [4] - l_rotation; \n",
    "        # [5] - l_scaling; [6] - w_translation\n",
    "\n",
    "        try:\n",
    "            print(\"\\nPlease select an option for the visualization:\")\n",
    "            print('[0] Original')\n",
    "            print('[1] Ground Truth Sampling (gt_sampling)')\n",
    "            print('[2] Flip along X-axis (flip_x)')\n",
    "            print('[3] World rotation (w_rotation)')\n",
    "            print('[4] Local rotation (l_rotation)')\n",
    "            print('[5] Local scaling (l_scaling)')\n",
    "            print('[6] World translation (w_translation)\\n')\n",
    "            user_choice = int(input('Your selection: '))\n",
    "            if user_choice not in range(7):\n",
    "                raise ValueError('Invalid selection')\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}. Please enter a number between 0 and 6.\")\n",
    "            logger.error(\"Invalid user input for the visualization mode.\")\n",
    "            return\n",
    "        \n",
    "        switch_cases = {0: \"original\", 1: \"gt_sampling\", 2: \"flip_x\", 3: \"w_rotation\", 4: \"l_rotation\", 5: \"l_scaling\", 6: \"w_translation\"}\n",
    "        \n",
    "        selected_data = data_list[user_choice]\n",
    "        points = selected_data['points']\n",
    "        gt_boxes = selected_data['gt_boxes']\n",
    "        gt_names = selected_data['gt_names']\n",
    "        gt_scores = selected_data['cam_info']['score']\n",
    "        lidar_idx = selected_data['frame_id']\n",
    "        print(lidar_idx)\n",
    "\n",
    "        if user_choice in [3, 4]: \n",
    "            noise_key = 'noise_world_rotation' if user_choice == 3 else 'noise_local_rotation'\n",
    "            noise_rot = selected_data.get(noise_key, None)\n",
    "            if noise_rot is not None:\n",
    "                logger.info(f'Noise rotation for {switch_cases[user_choice]}: {noise_rot}')\n",
    "        if user_choice == 5:\n",
    "            noise_scale = selected_data.get('noise_local_scaling', None)\n",
    "            if noise_scale is not None:\n",
    "                logger.info(f'Noise scaling for {switch_cases[user_choice]}: {noise_scale}')\n",
    "        if user_choice == 6:\n",
    "            noise_trans = selected_data.get('noise_world_translation', None)\n",
    "            if noise_trans is not None:\n",
    "                logger.info(f'Noise translation for {switch_cases[user_choice]}: {noise_trans}')\n",
    "        \n",
    "        Visualizer.draw_demo_scenes(points=points, gt_boxes=gt_boxes, gt_labels=gt_names, gt_score=gt_scores, point_colors=None, draw_origin=True, vc='')\n",
    "        logger.info('Demo visualization of training data done.')\n",
    "    \n",
    "    if get_ipython():\n",
    "        get_ipython().run_line_magic('reset', '-sf')\n",
    "        #get_ipython().kernel.do_shutdown(restart=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #main(mode='raw')\n",
    "    #main(mode='pre-processed')\n",
    "    main(mode='val')\n",
    "    #main(mode='train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcdet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
